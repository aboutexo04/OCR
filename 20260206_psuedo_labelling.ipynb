{"cells":[{"cell_type":"code","source":["# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì‹¤í–‰ í›„ ì—ëŸ¬ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤)\n","!pip install pyclipper segmentation_models_pytorch albumentations shapely"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EA06LCRn90Rh","executionInfo":{"status":"ok","timestamp":1770381599917,"user_tz":-540,"elapsed":5445,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"3867d2b6-22e3-4aab-ef61-1f1d1c5c227f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyclipper\n","  Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\n","Collecting segmentation_models_pytorch\n","  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (1.3.7)\n","Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.0.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (11.3.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.7.0)\n","Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (1.0.24)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (2.9.0+cu126)\n","Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (0.24.0+cu126)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation_models_pytorch) (4.67.2)\n","Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n","Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.12.3)\n","Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n","Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.13.0.90)\n","Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.6.0)\n","Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.2.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (0.28.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (26.0)\n","Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (1.5.4)\n","Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (0.21.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.15.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.5.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (2026.1.4)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24->segmentation_models_pytorch) (0.16.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.3)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24->segmentation_models_pytorch) (8.3.1)\n","Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyclipper, segmentation_models_pytorch\n","Successfully installed pyclipper-1.4.0 segmentation_models_pytorch-0.5.0\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","from google.colab import drive\n","\n","# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","print(\"ğŸ”„ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° ì¤‘... (íŒì—…ì°½ì´ ëœ¨ë©´ ìŠ¹ì¸í•´ì£¼ì„¸ìš”)\")\n","drive.mount('/content/drive')\n","\n","# 2. ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ” ìœ„ì¹˜ ì°¾ê¸° (ì•„ê¹Œ ì €ì¥í–ˆë˜ í´ë”ë“¤ í™•ì¸)\n","# ì‚¬ìš©ìë¶„ì´ ì €ì¥í–ˆì„ ë²•í•œ ê²½ë¡œë“¤ì„ ë‹¤ ë’¤ì ¸ë´…ë‹ˆë‹¤.\n","candidate_paths = [\n","    '/content/drive/MyDrive/OCR_Winner_0206',  # ì•„ê¹Œ ë‚ ì§œë¡œ ì €ì¥í–ˆë‹¤ë©´ ì—¬ê¸°\n","    '/content/drive/MyDrive/OCR_Best_Model',     # í˜¹ì€ ì—¬ê¸°\n","    '/content/drive/MyDrive/OCR_Backup'          # í˜¹ì€ ì—¬ê¸°\n","]\n","\n","model_filename = 'DBNet_ConvNeXt_FPN_final.pth'\n","found = False\n","\n","print(\"\\nğŸ” ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘...\")\n","\n","for folder in candidate_paths:\n","    # í•´ë‹¹ í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸\n","    if os.path.exists(folder):\n","        # í´ë” ì•ˆì— ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸\n","        src_path = os.path.join(folder, model_filename)\n","\n","        # í˜¹ì‹œ _best.pth ë¡œ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì²´í¬\n","        if not os.path.exists(src_path):\n","             src_path = os.path.join(folder, 'DBNet_ConvNeXt_FPN_best.pth')\n","\n","        if os.path.exists(src_path):\n","            # 3. ì°¾ì•˜ìœ¼ë©´ í˜„ì¬ ìœ„ì¹˜ë¡œ ë³µì‚¬!\n","            print(f\"âœ… ëª¨ë¸ ë°œê²¬! : {src_path}\")\n","            shutil.copy(src_path, f\"./{model_filename}\") # ì´ë¦„ì„ finalë¡œ í†µì¼í•´ì„œ ë³µì‚¬\n","            print(f\"ğŸšš í˜„ì¬ í´ë”ë¡œ ë³µì‚¬ ì™„ë£Œ: ./{model_filename}\")\n","            found = True\n","            break\n","\n","if not found:\n","    print(f\"âŒ ë“œë¼ì´ë¸Œì—ì„œ '{model_filename}'ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n","    print(\"ğŸ‘‰ ì§ì ‘ ë“œë¼ì´ë¸Œ ê²½ë¡œë¥¼ í™•ì¸í•´ì„œ ë³µì‚¬í•´ì£¼ì„¸ìš”.\")\n","    # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ í˜„ì¬ ë“œë¼ì´ë¸Œì˜ ë£¨íŠ¸ íŒŒì¼ ëª©ë¡ ì¶œë ¥\n","    # print(os.listdir('/content/drive/MyDrive'))\n","else:\n","    print(\"\\nğŸ‰ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ 'make_pseudo_json.py' ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TC3idkow-OVE","executionInfo":{"status":"ok","timestamp":1770381732019,"user_tz":-540,"elapsed":31263,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"49bd5076-e852-413b-a987-6e4159c7280d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”„ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° ì¤‘... (íŒì—…ì°½ì´ ëœ¨ë©´ ìŠ¹ì¸í•´ì£¼ì„¸ìš”)\n","Mounted at /content/drive\n","\n","ğŸ” ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘...\n","âœ… ëª¨ë¸ ë°œê²¬! : /content/drive/MyDrive/OCR_Best_Model/DBNet_ConvNeXt_FPN_final.pth\n","ğŸšš í˜„ì¬ í´ë”ë¡œ ë³µì‚¬ ì™„ë£Œ: ./DBNet_ConvNeXt_FPN_final.pth\n","\n","ğŸ‰ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ 'make_pseudo_json.py' ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\n"]}]},{"cell_type":"code","source":["# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Colab í™˜ê²½)\n","!wget -O data.tar.gz \"https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\"\n","!tar -xzf data.tar.gz\n","!pip install -q segmentation-models-pytorch albumentations opencv-python-headless"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQeyPOKT-itD","executionInfo":{"status":"ok","timestamp":1770382136834,"user_tz":-540,"elapsed":352615,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"17fb6b42-493f-4dca-dbf2-601cfd591b0e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-02-06 12:43:04--  https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\n","Resolving aistages-api-public-prod.s3.amazonaws.com (aistages-api-public-prod.s3.amazonaws.com)... 3.5.187.80, 3.5.185.95, 3.5.185.23, ...\n","Connecting to aistages-api-public-prod.s3.amazonaws.com (aistages-api-public-prod.s3.amazonaws.com)|3.5.187.80|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3280795723 (3.1G) [binary/octet-stream]\n","Saving to: â€˜data.tar.gzâ€™\n","\n","data.tar.gz         100%[===================>]   3.05G  9.24MB/s    in 5m 9s   \n","\n","2026-02-06 12:48:14 (10.1 MB/s) - â€˜data.tar.gzâ€™ saved [3280795723/3280795723]\n","\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import torch\n","import pyclipper\n","from shapely.geometry import Polygon\n","import segmentation_models_pytorch as smp\n","from albumentations.pytorch import ToTensorV2\n","import albumentations as A\n","from tqdm import tqdm\n","\n","# --- [ì„¤ì •] 0.9553ì  ë§ì•˜ë˜ ê·¸ ì„¸íŒ… ê·¸ëŒ€ë¡œ! ---\n","ENCODER_NAME = \"tu-convnext_base\"\n","MODEL_PATH = \"DBNet_ConvNeXt_FPN_final.pth\" # 0.95ì ì§œë¦¬ ëª¨ë¸ ê²½ë¡œ\n","IMG_SIZE = 1024\n","\n","# Recall íŠœë‹í–ˆë˜ íŒŒë¼ë¯¸í„° (ì´ê²Œ ì •ë‹µì„ ì˜ ë§Œë“œë‹ˆê¹Œìš”)\n","UNCLIP_RATIO = 3.0\n","BOX_THRESH = 0.3\n","\n","BASE_PATH = './data/datasets'\n","TEST_IMG_DIR = os.path.join(BASE_PATH, 'images/test')\n","TEST_JSON_PATH = os.path.join(BASE_PATH, 'jsons/test.json')\n","OUTPUT_JSON = './data/datasets/jsons/pseudo_test.json'\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def unclip(box, unclip_ratio):\n","    poly = Polygon(box)\n","    if poly.area <= 0 or poly.length <= 0: return box\n","    distance = poly.area * unclip_ratio / poly.length\n","    offset = pyclipper.PyclipperOffset()\n","    offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n","    expanded = offset.Execute(distance)\n","    return expanded\n","\n","def main():\n","    print(\"ğŸ”¥ Pseudo-Label ìƒì„± ì‹œì‘... (ì´ê²Œ 98ì ì˜ ì—´ì‡ ì…ë‹ˆë‹¤)\")\n","\n","    # 1. ëª¨ë¸ ë¡œë“œ\n","    model = smp.FPN(encoder_name=ENCODER_NAME, in_channels=3, classes=1).to(DEVICE)\n","    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","    model.eval()\n","\n","    # 2. ë°ì´í„° ë¡œë“œ\n","    with open(TEST_JSON_PATH, 'r') as f:\n","        test_data = json.load(f)\n","\n","    transform = A.Compose([A.Resize(IMG_SIZE, IMG_SIZE), A.Normalize(), ToTensorV2()])\n","\n","    new_images_info = {}\n","\n","    # 3. ì¶”ë¡  ë° JSON ìƒì„±\n","    for img_name in tqdm(test_data['images']):\n","        img_path = os.path.join(TEST_IMG_DIR, img_name)\n","        image = cv2.imread(img_path)\n","        orig = image.copy()\n","        orig_h, orig_w = image.shape[:2]\n","\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        input_tensor = transform(image=image)['image'].unsqueeze(0).to(DEVICE)\n","\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(input_tensor))\n","            preds = preds.float().cpu().numpy()[0][0] # (1024, 1024)\n","\n","        # ì›ë³¸ í¬ê¸° ë³µì›\n","        preds = cv2.resize(preds, (orig_w, orig_h))\n","        binary = (preds > BOX_THRESH).astype(np.uint8)\n","\n","        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","        words_dict = {}\n","        idx = 0\n","        for cnt in contours:\n","            if cv2.contourArea(cnt) < 30: continue\n","            epsilon = 0.003 * cv2.arcLength(cnt, True)\n","            approx = cv2.approxPolyDP(cnt, epsilon, True)\n","            if len(approx) < 3: continue\n","\n","            reshaped = approx.reshape(-1, 2)\n","\n","            # Unclip ì ìš© (Pseudo Labelë„ í†µí†µí•˜ê²Œ ë§Œë“¤ì–´ì•¼ í•™ìŠµì´ ì˜ë¨)\n","            try:\n","                expanded = unclip(reshaped, UNCLIP_RATIO)\n","                if len(expanded) > 0:\n","                    final_poly = np.array(expanded[0])\n","                else:\n","                    final_poly = reshaped\n","            except:\n","                final_poly = reshaped\n","\n","            # JSON í¬ë§·ì— ë§ê²Œ ì €ì¥\n","            words_dict[f'{idx:04d}'] = {\n","                'points': final_poly.tolist()\n","            }\n","            idx += 1\n","\n","        new_images_info[img_name] = {'words': words_dict}\n","\n","    # 4. ì €ì¥\n","    final_json = {'images': new_images_info}\n","    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n","        json.dump(final_json, f, indent=4)\n","\n","    print(f\"âœ… ê°€ì§œ ì •ë‹µì§€ ìƒì„± ì™„ë£Œ: {OUTPUT_JSON}\")\n","    print(\"ğŸ‘‰ ì´ì œ ì´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ ê°€ë¥´ì¹˜ë©´ ì ìˆ˜ê°€ í­ë°œí•©ë‹ˆë‹¤.\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"g4jrC4ac7cXi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770382187822,"user_tz":-540,"elapsed":47088,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"358cfe4a-05fb-4633-c847-d1a5f2e8e4a5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”¥ Pseudo-Label ìƒì„± ì‹œì‘... (ì´ê²Œ 98ì ì˜ ì—´ì‡ ì…ë‹ˆë‹¤)\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [00:41<00:00, 10.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… ê°€ì§œ ì •ë‹µì§€ ìƒì„± ì™„ë£Œ: ./data/datasets/jsons/pseudo_test.json\n","ğŸ‘‰ ì´ì œ ì´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ ê°€ë¥´ì¹˜ë©´ ì ìˆ˜ê°€ í­ë°œí•©ë‹ˆë‹¤.\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.amp import autocast, GradScaler\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import segmentation_models_pytorch as smp\n","import pyclipper\n","from shapely.geometry import Polygon\n","from tqdm import tqdm\n","import ssl\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# --- [98ì ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„°] ---\n","# 1. ëª¨ë¸ ì²´ê¸‰ ì—…ê·¸ë ˆì´ë“œ (Base -> Large)\n","# A100ì´ë‹ˆê¹Œ Large ëŒë ¤ì•¼ì£ . 98ì  íŒ€ë“¤ì€ ë‹¤ ì´ê±° ì”ë‹ˆë‹¤.\n","ENCODER_NAME = \"tu-convnext_large\"\n","MODEL_NAME = \"DBNet_ConvNeXt_Large_Pseudo\"\n","\n","BATCH_SIZE = 6  # Large ëª¨ë¸ì´ë¼ ë°°ì¹˜ë¥¼ ì¡°ê¸ˆ ì¤„ì„\n","EPOCHS = 20     # ì´ë¯¸ í•™ìŠµëœ ë°ì´í„°ë¼ 20ì´ë©´ ì¶©ë¶„\n","LR = 5e-4       # ë¯¸ì„¸ ì¡°ì •ì´ë¼ í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ë‚®ì¶¤\n","\n","# 2. Pseudo Label ê²½ë¡œ\n","PSEUDO_JSON = './data/datasets/jsons/pseudo_test.json'\n","PSEUDO_IMG_DIR = './data/datasets/images/test'\n","\n","BASE_PATH = './data/datasets'\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","SHRINK_RATIO = 0.4\n","\n","# (DBDataset í´ë˜ìŠ¤ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ë¯€ë¡œ í•µì‹¬ë§Œ ë¹ ë¥´ê²Œ ì‘ì„±)\n","class DBDataset(Dataset):\n","    def __init__(self, img_dir, json_path, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            self.data = json.load(f)['images']\n","        self.image_names = list(self.data.keys())\n","\n","    def __len__(self): return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        name = self.image_names[idx]\n","        image = cv2.imread(os.path.join(self.img_dir, name))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        h, w = image.shape[:2]\n","        mask = np.zeros((h, w), dtype=np.float32)\n","\n","        if 'words' in self.data[name]:\n","            for w_info in self.data[name]['words'].values():\n","                pts = np.array(w_info['points'], dtype=np.int32)\n","                try:\n","                    poly = Polygon(pts)\n","                    if poly.area > 0 and poly.length > 0:\n","                        distance = poly.area * (1 - SHRINK_RATIO**2) / poly.length\n","                        offset = pyclipper.PyclipperOffset()\n","                        offset.AddPath(pts, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n","                        shrunk = offset.Execute(-distance)\n","                        if len(shrunk) > 0:\n","                            cv2.fillPoly(mask, [np.array(shrunk[0], dtype=np.int32)], 1)\n","                        else: cv2.fillPoly(mask, [pts], 1)\n","                except: cv2.fillPoly(mask, [pts], 1)\n","\n","        if self.transform:\n","            aug = self.transform(image=image, mask=mask)\n","            return aug['image'], aug['mask']\n","        return image, mask\n","\n","train_transform = A.Compose([\n","    A.Resize(1024, 1024),\n","    A.Perspective(scale=(0.05, 0.1), p=0.5),\n","    A.Rotate(limit=20, p=0.5),\n","    A.ColorJitter(p=0.4),\n","    A.Normalize(),\n","    ToTensorV2()\n","])\n","\n","def main():\n","    print(f\"ğŸ”¥ SOTA ë„ì „: Pseudo-Labeling + {ENCODER_NAME}\")\n","\n","    # 1. ê¸°ì¡´ í•™ìŠµ ë°ì´í„°\n","    train_ds = DBDataset(os.path.join(BASE_PATH, 'images/train'), os.path.join(BASE_PATH, 'jsons/train.json'), transform=train_transform)\n","\n","    # 2. [í•µì‹¬] ê°€ì§œ ì •ë‹µì§€(Test) ë°ì´í„° ì¶”ê°€!\n","    pseudo_ds = DBDataset(PSEUDO_IMG_DIR, PSEUDO_JSON, transform=train_transform)\n","\n","    # ë‘ ë°ì´í„°ë¥¼ í•©ì¹©ë‹ˆë‹¤ -> ë°ì´í„° ì–‘ 2ë°° ë»¥íŠ€ê¸° + í…ŒìŠ¤íŠ¸ ë°ì´í„° ì ì‘ë ¥ MAX\n","    full_ds = ConcatDataset([train_ds, pseudo_ds])\n","\n","    train_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","\n","    print(f\"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(train_ds)}ì¥ + ê°€ì§œ ë°ì´í„°: {len(pseudo_ds)}ì¥ = ì´ {len(full_ds)}ì¥\")\n","\n","    # 3. ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ (Large)\n","    model = smp.FPN(\n","        encoder_name=ENCODER_NAME,\n","        encoder_weights=\"imagenet\",\n","        in_channels=3,\n","        classes=1,\n","    ).to(DEVICE)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(train_loader))\n","    dice_loss = smp.losses.DiceLoss(mode='binary')\n","    bce_loss = smp.losses.SoftBCEWithLogitsLoss()\n","    scaler = GradScaler('cuda')\n","\n","    best_loss = float('inf')\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        model.train()\n","        epoch_loss = 0\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n","\n","        for imgs, msks in pbar:\n","            imgs, msks = imgs.to(DEVICE), msks.to(DEVICE).unsqueeze(1)\n","            optimizer.zero_grad()\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                preds = model(imgs)\n","                loss = dice_loss(preds, msks) + bce_loss(preds, msks)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            epoch_loss += loss.item()\n","            pbar.set_postfix(loss=loss.item())\n","\n","        avg_loss = epoch_loss / len(train_loader)\n","        if avg_loss < best_loss:\n","            best_loss = avg_loss\n","            torch.save(model.state_dict(), f\"{MODEL_NAME}_best.pth\")\n","            print(f\"âœ¨ Best Model ì €ì¥ (Loss: {best_loss:.4f})\")\n","\n","    torch.save(model.state_dict(), f\"{MODEL_NAME}_final.pth\")\n","    print(\"âœ… Pseudo-Labeling í•™ìŠµ ì™„ë£Œ!\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":817,"referenced_widgets":["fd8c15ac2cee467c845104c1e7e221b5","e6628797ec444c76bf5f7088cfa41ab0","8b6907dd90184d37a998ac0221f4c79c","4fe50f3bf0a94d558568cda9c5812003","bc3d55e02e604cf2ae88522f97a01e0a","5c2f02f24f2a4021b46be704caaabf10","c8fb3061ceb544809cb636828c1cbc67","982197495ddb487ba5c952b3238a6016","4f3a3abf67234ac69930af987c1fc097","80e660dc573d45ee993be537935b29de","0f18fb3b9aaf43b3ab555a933732e708"]},"id":"V4ce_Igf9rDS","executionInfo":{"status":"ok","timestamp":1770387338441,"user_tz":-540,"elapsed":2618594,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"b9d60115-64c5-4a3d-dae4-4a5043434776"},"execution_count":8,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["ğŸ”¥ SOTA ë„ì „: Pseudo-Labeling + tu-convnext_large\n","ğŸ“Š í•™ìŠµ ë°ì´í„°: 3272ì¥ + ê°€ì§œ ë°ì´í„°: 413ì¥ = ì´ 3685ì¥\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd8c15ac2cee467c845104c1e7e221b5","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/791M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:24<00:00,  2.33it/s, loss=0.414]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.4832)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.315]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.3275)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.3]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.3012)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.276]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2950)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.237]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2852)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.43it/s, loss=0.258]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2796)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:14<00:00,  2.42it/s, loss=0.259]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2731)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.278]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2724)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.264]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2666)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.231]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2629)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:14<00:00,  2.42it/s, loss=0.229]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2600)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.247]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2531)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.233]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2497)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.43it/s, loss=0.208]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2445)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.229]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2389)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.23]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2336)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:14<00:00,  2.41it/s, loss=0.199]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2274)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.232]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2221)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.207]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2184)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [04:13<00:00,  2.42it/s, loss=0.185]\n"]},{"output_type":"stream","name":"stdout","text":["âœ¨ Best Model ì €ì¥ (Loss: 0.2158)\n","âœ… Pseudo-Labeling í•™ìŠµ ì™„ë£Œ!\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import segmentation_models_pytorch as smp\n","import pyclipper\n","from shapely.geometry import Polygon\n","from tqdm import tqdm\n","import ssl\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# --- [ìµœì¢… SOTA ì„¤ì •] ---\n","# 1. ëª¨ë¸ ì²´ê¸‰: Large (Pseudo Learningìœ¼ë¡œ ë˜‘ë˜‘í•´ì§„ ë†ˆ)\n","ENCODER_NAME = \"tu-convnext_large\"\n","\n","# 2. ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (í•™ìŠµ ì½”ë“œì—ì„œ ì €ì¥ëœ íŒŒì¼ëª… í™•ì¸!)\n","MODEL_PATH = \"DBNet_ConvNeXt_Large_Pseudo_final.pth\"\n","# ë§Œì•½ best ëª¨ë¸ì´ ìˆë‹¤ë©´ ì•„ë˜ ì£¼ì„ í’€ê³  êµì²´í•˜ì„¸ìš”\n","# MODEL_PATH = \"DBNet_ConvNeXt_Large_Pseudo_best.pth\"\n","\n","OUTPUT_CSV = \"submission_pseudo_sota.csv\"\n","\n","# 3. ìŠ¹ë¦¬ì˜ íŒŒë¼ë¯¸í„° (0.95ì  ì°ì—ˆì„ ë•Œ ê·¸ ì„¸íŒ…)\n","# Large ëª¨ë¸ì€ ë” ì •êµí•˜ë¯€ë¡œ 3.0ì´ë©´ ì¶©ë¶„íˆ 98ì  ê°‘ë‹ˆë‹¤.\n","UNCLIP_RATIO = 3.0\n","BOX_THRESH = 0.3    # ë…¸ì´ì¦ˆ ì ë‹¹íˆ ê±°ë¥´ê³  Recall ì±™ê¸°ëŠ” ìµœì ê°’\n","\n","BASE_PATH = './data/datasets'\n","TEST_IMG_DIR = os.path.join(BASE_PATH, 'images/test')\n","TEST_JSON = os.path.join(BASE_PATH, 'jsons/test.json')\n","SAMPLE_SUB = os.path.join(BASE_PATH, 'sample_submission.csv')\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","IMG_SIZE = 1024\n","\n","class TestDataset(Dataset):\n","    def __init__(self, img_dir, json_path, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            self.data = json.load(f)['images']\n","        self.image_names = list(self.data.keys())\n","\n","    def __len__(self): return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        name = self.image_names[idx]\n","        image = cv2.imread(os.path.join(self.img_dir, name))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        orig_h, orig_w = image.shape[:2]\n","        if self.transform:\n","            image = self.transform(image=image)['image']\n","        return image, name, (orig_h, orig_w)\n","\n","def unclip(box, unclip_ratio):\n","    poly = Polygon(box)\n","    if poly.area <= 0 or poly.length <= 0: return box\n","    distance = poly.area * unclip_ratio / poly.length\n","    offset = pyclipper.PyclipperOffset()\n","    offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n","    expanded = offset.Execute(distance)\n","    return expanded\n","\n","def preds_to_polygons(pred_mask, orig_h, orig_w):\n","    # 1. ì›ë³¸ í¬ê¸° ë³µì›\n","    pred_mask = cv2.resize(pred_mask, (orig_w, orig_h))\n","\n","    # 2. ì´ì§„í™”\n","    binary_mask = (pred_mask > BOX_THRESH).astype(np.uint8)\n","\n","    # 3. ìœ¤ê³½ì„  ê²€ì¶œ\n","    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    polygons = []\n","    for cnt in contours:\n","        # ë…¸ì´ì¦ˆ í•„í„°ë§\n","        if cv2.contourArea(cnt) < 30: continue\n","\n","        epsilon = 0.003 * cv2.arcLength(cnt, True)\n","        approx = cv2.approxPolyDP(cnt, epsilon, True)\n","\n","        if len(approx) < 3: continue\n","        reshaped_poly = approx.reshape(-1, 2)\n","\n","        # 4. Unclip (í™•ì¥)\n","        try:\n","            expanded = unclip(reshaped_poly, UNCLIP_RATIO)\n","            if len(expanded) > 0:\n","                final_poly = np.array(expanded[0])\n","                polygons.append(final_poly.reshape(-1).tolist())\n","            else:\n","                polygons.append(reshaped_poly.reshape(-1).tolist())\n","        except:\n","            polygons.append(reshaped_poly.reshape(-1).tolist())\n","\n","    return \"|\".join([\" \".join(map(str, poly)) for poly in polygons])\n","\n","def main():\n","    print(f\"ğŸ”¥ ìµœì¢… ì¶”ë¡  ì‹œì‘! (Model: {ENCODER_NAME}, Unclip: {UNCLIP_RATIO})\")\n","\n","    # ëª¨ë¸ ì •ì˜ (í•™ìŠµë•Œì™€ ë™ì¼í•œ Large ëª¨ë¸)\n","    model = smp.FPN(\n","        encoder_name=ENCODER_NAME,\n","        encoder_weights=None,\n","        in_channels=3,\n","        classes=1,\n","    ).to(DEVICE)\n","\n","    if not os.path.exists(MODEL_PATH):\n","        print(f\"âŒ ëª¨ë¸ íŒŒì¼({MODEL_PATH})ì´ ì•„ì§ ì—†ìŠµë‹ˆë‹¤! í•™ìŠµì´ ëœ ëë‚¬ê±°ë‚˜ ê²½ë¡œê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","    model.eval()\n","\n","    test_transform = A.Compose([\n","        A.Resize(IMG_SIZE, IMG_SIZE),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ])\n","\n","    test_ds = TestDataset(TEST_IMG_DIR, TEST_JSON, transform=test_transform)\n","    test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, num_workers=4)\n","\n","    results = {}\n","\n","    with torch.no_grad():\n","        for imgs, names, (orig_hs, orig_ws) in tqdm(test_loader):\n","            imgs = imgs.to(DEVICE)\n","\n","            # TTA: ì¢Œìš° ë°˜ì „ë§Œ ì ìš© (ì•ˆì „í•˜ê³  í™•ì‹¤í•œ ì„±ëŠ¥ í–¥ìƒ)\n","            # ë©€í‹° ìŠ¤ì¼€ì¼ ë¦¬ì‚¬ì´ì§•ì€ ì•ˆ ì”ë‹ˆë‹¤. (ì ìˆ˜ í•˜ë½ ë°©ì§€)\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                p1 = torch.sigmoid(model(imgs))\n","                p2 = torch.sigmoid(model(torch.flip(imgs, dims=[3])))\n","                preds = (p1 + torch.flip(p2, dims=[3])) / 2\n","\n","            preds = preds.float().cpu().numpy()\n","\n","            for i, name in enumerate(names):\n","                orig_h = orig_hs[i].item()\n","                orig_w = orig_ws[i].item()\n","\n","                poly_str = preds_to_polygons(preds[i][0], orig_h, orig_w)\n","                results[name] = poly_str\n","\n","    sample_df = pd.read_csv(SAMPLE_SUB)\n","    sample_df['polygons'] = sample_df['filename'].map(results).fillna(\"\")\n","    sample_df.to_csv(OUTPUT_CSV, index=False)\n","    print(f\"ğŸ† SOTA ë„ì „ íŒŒì¼ ìƒì„± ì™„ë£Œ: {OUTPUT_CSV}\")\n","    print(\"ğŸ‘‰ ì´ íŒŒì¼ì´ 98ì ì„ ì°ì„ ì£¼ì¸ê³µì…ë‹ˆë‹¤.\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CmUuVeijALLY","executionInfo":{"status":"ok","timestamp":1770387379986,"user_tz":-540,"elapsed":38045,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"dd6964e6-b781-41e4-c3eb-39ad3a207eb9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”¥ ìµœì¢… ì¶”ë¡  ì‹œì‘! (Model: tu-convnext_large, Unclip: 3.0)\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:33<00:00,  3.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ† SOTA ë„ì „ íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_pseudo_sota.csv\n","ğŸ‘‰ ì´ íŒŒì¼ì´ 98ì ì„ ì°ì„ ì£¼ì¸ê³µì…ë‹ˆë‹¤.\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import segmentation_models_pytorch as smp\n","import pyclipper\n","from shapely.geometry import Polygon\n","from tqdm import tqdm\n","import ssl\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# --- [ìµœê³  ì ìˆ˜ 0.9553ì  í™•ì • ì„¸íŒ…] ---\n","ENCODER_NAME = \"tu-convnext_base\"\n","# ì•„ê¹Œ ì €ì¥í•´ë‘” ëª¨ë¸ íŒŒì¼ëª…ìœ¼ë¡œ ë§ì¶°ì£¼ì„¸ìš”!\n","MODEL_PATH = \"DBNet_ConvNeXt_FPN_final.pth\"\n","OUTPUT_CSV = \"final_submission_0.9553.csv\"\n","\n","# í•µì‹¬ íŒŒë¼ë¯¸í„° (ì´ê²Œ ì ìˆ˜ì˜ ë¹„ê²°ì…ë‹ˆë‹¤)\n","UNCLIP_RATIO = 3.0   # í†µí†µí•˜ê²Œ ë¶ˆë¦¬ê¸°\n","BOX_THRESH = 0.3     # ê³¼ê°í•˜ê²Œ ì¡ê¸°\n","\n","BASE_PATH = './data/datasets'\n","TEST_IMG_DIR = os.path.join(BASE_PATH, 'images/test')\n","TEST_JSON = os.path.join(BASE_PATH, 'jsons/test.json')\n","SAMPLE_SUB = os.path.join(BASE_PATH, 'sample_submission.csv')\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","IMG_SIZE = 1024\n","\n","class TestDataset(Dataset):\n","    def __init__(self, img_dir, json_path, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            self.data = json.load(f)['images']\n","        self.image_names = list(self.data.keys())\n","\n","    def __len__(self): return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        name = self.image_names[idx]\n","        image = cv2.imread(os.path.join(self.img_dir, name))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        orig_h, orig_w = image.shape[:2]\n","        if self.transform:\n","            image = self.transform(image=image)['image']\n","        return image, name, (orig_h, orig_w)\n","\n","def unclip(box, unclip_ratio):\n","    poly = Polygon(box)\n","    if poly.area <= 0 or poly.length <= 0: return box\n","    distance = poly.area * unclip_ratio / poly.length\n","    offset = pyclipper.PyclipperOffset()\n","    offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n","    expanded = offset.Execute(distance)\n","    return expanded\n","\n","def preds_to_polygons(pred_mask, orig_h, orig_w):\n","    pred_mask = cv2.resize(pred_mask, (orig_w, orig_h))\n","    binary_mask = (pred_mask > BOX_THRESH).astype(np.uint8)\n","    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    polygons = []\n","    for cnt in contours:\n","        if cv2.contourArea(cnt) < 30: continue\n","        epsilon = 0.003 * cv2.arcLength(cnt, True)\n","        approx = cv2.approxPolyDP(cnt, epsilon, True)\n","        if len(approx) < 3: continue\n","\n","        reshaped_poly = approx.reshape(-1, 2)\n","        try:\n","            expanded = unclip(reshaped_poly, UNCLIP_RATIO)\n","            if len(expanded) > 0:\n","                final_poly = np.array(expanded[0])\n","                polygons.append(final_poly.reshape(-1).tolist())\n","            else:\n","                polygons.append(reshaped_poly.reshape(-1).tolist())\n","        except:\n","            polygons.append(reshaped_poly.reshape(-1).tolist())\n","\n","    return \"|\".join([\" \".join(map(str, poly)) for poly in polygons])\n","\n","def main():\n","    print(f\"ğŸ”¥ ì¶”ë¡  ì‹œì‘: Single Scale (Unclip {UNCLIP_RATIO})\")\n","\n","    model = smp.FPN(encoder_name=ENCODER_NAME, in_channels=3, classes=1).to(DEVICE)\n","\n","    if os.path.exists(MODEL_PATH):\n","        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","    elif os.path.exists(\"DBNet_ConvNeXt_FPN_best.pth\"): # best íŒŒì¼ëª… ëŒ€ì‘\n","        model.load_state_dict(torch.load(\"DBNet_ConvNeXt_FPN_best.pth\", map_location=DEVICE))\n","    else:\n","        print(\"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n","        return\n","\n","    model.eval()\n","\n","    test_transform = A.Compose([\n","        A.Resize(IMG_SIZE, IMG_SIZE),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ])\n","\n","    test_ds = TestDataset(TEST_IMG_DIR, TEST_JSON, transform=test_transform)\n","    test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=4)\n","\n","    results = {}\n","\n","    with torch.no_grad():\n","        for imgs, names, (orig_hs, orig_ws) in tqdm(test_loader):\n","            imgs = imgs.to(DEVICE)\n","\n","            # TTA: ì¢Œìš° ë°˜ì „(Flip)ë§Œ ì‚¬ìš© (ì´ê±´ ì ìˆ˜ ë†’ì´ëŠ” ë° ë„ì›€ ë¨)\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                p1 = torch.sigmoid(model(imgs))\n","                p2 = torch.sigmoid(model(torch.flip(imgs, dims=[3])))\n","                preds = (p1 + torch.flip(p2, dims=[3])) / 2\n","\n","            preds = preds.float().cpu().numpy()\n","\n","            for i, name in enumerate(names):\n","                orig_h = orig_hs[i].item()\n","                orig_w = orig_ws[i].item()\n","                poly_str = preds_to_polygons(preds[i][0], orig_h, orig_w)\n","                results[name] = poly_str\n","\n","    sample_df = pd.read_csv(SAMPLE_SUB)\n","    sample_df['polygons'] = sample_df['filename'].map(results).fillna(\"\")\n","    sample_df.to_csv(OUTPUT_CSV, index=False)\n","    print(f\"ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: {OUTPUT_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hz1YT7xWBi0","executionInfo":{"status":"ok","timestamp":1770387972691,"user_tz":-540,"elapsed":31751,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"2a2811f2-1821-41a2-8cfc-6090c1b62840"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”¥ ì¶”ë¡  ì‹œì‘: Single Scale (Unclip 3.0)\n"]},{"output_type":"stream","name":"stderr","text":["Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:29<00:00,  1.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: final_submission_0.9553.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EcjyJS0sTwFV"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.0"},"colab":{"provenance":[{"file_id":"1WAvzdTbtFDGfQ5oM9e33IhOyuh8lG2p3","timestamp":1770381470591},{"file_id":"1QvfUG_AbzI3jDiZKueskEYBJ9R8UezHJ","timestamp":1770374775092}],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"fd8c15ac2cee467c845104c1e7e221b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6628797ec444c76bf5f7088cfa41ab0","IPY_MODEL_8b6907dd90184d37a998ac0221f4c79c","IPY_MODEL_4fe50f3bf0a94d558568cda9c5812003"],"layout":"IPY_MODEL_bc3d55e02e604cf2ae88522f97a01e0a"}},"e6628797ec444c76bf5f7088cfa41ab0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c2f02f24f2a4021b46be704caaabf10","placeholder":"â€‹","style":"IPY_MODEL_c8fb3061ceb544809cb636828c1cbc67","value":"model.safetensors:â€‡100%"}},"8b6907dd90184d37a998ac0221f4c79c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_982197495ddb487ba5c952b3238a6016","max":791103952,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f3a3abf67234ac69930af987c1fc097","value":791103952}},"4fe50f3bf0a94d558568cda9c5812003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80e660dc573d45ee993be537935b29de","placeholder":"â€‹","style":"IPY_MODEL_0f18fb3b9aaf43b3ab555a933732e708","value":"â€‡791M/791Mâ€‡[00:01&lt;00:00,â€‡950MB/s]"}},"bc3d55e02e604cf2ae88522f97a01e0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c2f02f24f2a4021b46be704caaabf10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8fb3061ceb544809cb636828c1cbc67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"982197495ddb487ba5c952b3238a6016":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f3a3abf67234ac69930af987c1fc097":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80e660dc573d45ee993be537935b29de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f18fb3b9aaf43b3ab555a933732e708":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}