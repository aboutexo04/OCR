{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P54spJCvKADq","executionInfo":{"status":"ok","timestamp":1770374990097,"user_tz":-540,"elapsed":114424,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"983e9ea6-5cd6-4cf8-e6ae-bd9a78e571a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-02-06 10:47:55--  https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\n","Resolving aistages-api-public-prod.s3.amazonaws.com (aistages-api-public-prod.s3.amazonaws.com)... 3.5.185.69, 52.219.146.51, 3.5.187.10, ...\n","Connecting to aistages-api-public-prod.s3.amazonaws.com (aistages-api-public-prod.s3.amazonaws.com)|3.5.185.69|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3280795723 (3.1G) [binary/octet-stream]\n","Saving to: â€˜data.tar.gzâ€™\n","\n","data.tar.gz         100%[===================>]   3.05G  47.3MB/s    in 71s     \n","\n","2026-02-06 10:49:06 (44.3 MB/s) - â€˜data.tar.gzâ€™ saved [3280795723/3280795723]\n","\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Colab í™˜ê²½)\n","!wget -O data.tar.gz \"https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\"\n","!tar -xzf data.tar.gz\n","!pip install -q segmentation-models-pytorch albumentations opencv-python-headless"]},{"cell_type":"code","source":["!pip install pyclipper shapely"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2YE8Lmdkwro","executionInfo":{"status":"ok","timestamp":1770375036514,"user_tz":-540,"elapsed":4509,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"6dc6ab2b-d7c0-4c4e-c89d-7fde93934e79"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyclipper\n","  Using cached pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n","Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from shapely) (2.0.2)\n","Using cached pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n","Installing collected packages: pyclipper\n","Successfully installed pyclipper-1.4.0\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from torch.amp import autocast, GradScaler\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import segmentation_models_pytorch as smp\n","import pyclipper\n","from shapely.geometry import Polygon\n","from tqdm import tqdm\n","import ssl\n","import warnings\n","\n","# --- í™˜ê²½ ì„¤ì • ---\n","warnings.filterwarnings('ignore')\n","ssl._create_default_https_context = ssl._create_unverified_context\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# --- [í•˜ì´í¼íŒŒë¼ë¯¸í„°] ---\n","# 1. ëª¨ë¸ ì„¤ì •\n","ENCODER_NAME = \"tu-convnext_base\" # ì„±ëŠ¥ ìµœê°• ë°±ë³¸\n","MODEL_NAME = \"DBNet_ConvNeXt_FPN\"\n","\n","# 2. í•™ìŠµ ì„¤ì •\n","IMG_SIZE = 1024\n","BATCH_SIZE = 8\n","EPOCHS = 25\n","LR = 1e-3\n","\n","# 3. DBNet í•µì‹¬ íŒŒë¼ë¯¸í„° (ì‹ ì˜ í•œ ìˆ˜)\n","SHRINK_RATIO = 0.4  # í•™ìŠµ ì‹œ: ê¸€ì ì˜ì—­ì„ 40% ìª¼ê·¸ë¼íŠ¸ë¦¼ (ë¶„ë¦¬ ìœ ë„)\n","UNCLIP_RATIO = 1.8  # ì¶”ë¡  ì‹œ: ìª¼ê·¸ë¼ë“ ê±¸ 1.8ë°° ë‹¤ì‹œ í‚¤ì›€ (ì˜ì—­ í™•ë³´)\n","BOX_THRESH = 0.5    # í™•ì‹ ë„ 0.5 ì´ìƒë§Œ ì¸ì •\n","\n","# --- [ê²½ë¡œ ì„¤ì •] ---\n","BASE_PATH = './data/datasets'\n","PSEUDO_PATH = './data/pseudo_label'\n","OUTPUT_CSV = 'submission_dbnet_final.csv'\n","\n","# ==========================================\n","# 1. Dataset í´ë˜ìŠ¤ (í•™ìŠµìš© & ì¶”ë¡ ìš©)\n","# ==========================================\n","class DBDataset(Dataset):\n","    def __init__(self, img_dir, json_path, transform=None, mode='train'):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.mode = mode\n","\n","        # JSON ë¡œë“œ (ì—†ìœ¼ë©´ ì˜ˆì™¸ì²˜ë¦¬)\n","        if json_path and not os.path.exists(json_path):\n","            alt = json_path.replace('train.json', 'val.json')\n","            if os.path.exists(alt): json_path = alt\n","            else:\n","                self.image_names = []\n","                return\n","\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            self.data = json.load(f)['images']\n","        self.image_names = list(self.data.keys())\n","\n","    def __len__(self): return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        name = self.image_names[idx]\n","        image = cv2.imread(os.path.join(self.img_dir, name))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        # ì¶”ë¡  ëª¨ë“œì¼ ê²½ìš° ì›ë³¸ í¬ê¸° ì €ì¥ í•„ìš”\n","        orig_h, orig_w = image.shape[:2]\n","\n","        # ë§ˆìŠ¤í¬ ìƒì„± (í•™ìŠµìš©)\n","        mask = np.zeros((orig_h, orig_w), dtype=np.float32)\n","\n","        if self.mode == 'train' and 'words' in self.data[name]:\n","            for w_info in self.data[name]['words'].values():\n","                pts = np.array(w_info['points'], dtype=np.int32)\n","\n","                # [DBNet í•™ìŠµì˜ í•µì‹¬: Shrink]\n","                # ê¸€ìë¥¼ ì•ˆìª½ìœ¼ë¡œ ì¶•ì†Œì‹œì¼œì„œ ë§ˆìŠ¤í¬ë¥¼ ë§Œë“¦ -> ì¸ì ‘ ê¸€ì ë¶„ë¦¬ íš¨ê³¼\n","                try:\n","                    poly = Polygon(pts)\n","                    area = poly.area\n","                    perimeter = poly.length\n","\n","                    if area > 0 and perimeter > 0:\n","                        distance = area * (1 - SHRINK_RATIO * SHRINK_RATIO) / perimeter\n","                        offset = pyclipper.PyclipperOffset()\n","                        offset.AddPath(pts, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n","                        shrunk = offset.Execute(-distance) # ë§ˆì´ë„ˆìŠ¤ ê±°ë¦¬ = ì¶•ì†Œ\n","\n","                        if len(shrunk) > 0:\n","                            cv2.fillPoly(mask, [np.array(shrunk[0], dtype=np.int32)], 1)\n","                        else:\n","                            cv2.fillPoly(mask, [pts], 1) # ë„ˆë¬´ ì‘ì•„ì§€ë©´ ì›ë³¸ ì‚¬ìš©\n","                except:\n","                    cv2.fillPoly(mask, [pts], 1)\n","\n","        if self.transform:\n","            aug = self.transform(image=image, mask=mask)\n","            image = aug['image']\n","            mask = aug['mask']\n","\n","        if self.mode == 'test':\n","            return image, name, (orig_h, orig_w)\n","        else:\n","            return image, mask\n","\n","# ==========================================\n","# 2. Augmentation\n","# ==========================================\n","train_transform = A.Compose([\n","    A.Resize(IMG_SIZE, IMG_SIZE),\n","    A.Perspective(scale=(0.05, 0.1), p=0.5),\n","    A.Rotate(limit=20, p=0.5),\n","    A.ColorJitter(brightness=0.2, contrast=0.2, p=0.4),\n","    A.Normalize(),\n","    ToTensorV2()\n","])\n","\n","test_transform = A.Compose([\n","    A.Resize(IMG_SIZE, IMG_SIZE),\n","    A.Normalize(),\n","    ToTensorV2()\n","])\n","\n","# ==========================================\n","# 3. í•™ìŠµ í•¨ìˆ˜\n","# ==========================================\n","def train():\n","    print(f\"ğŸ”¥ DBNet í•™ìŠµ ì‹œì‘! (Encoder: {ENCODER_NAME})\")\n","    print(f\"   - Shrink Ratio: {SHRINK_RATIO} (ê¸€ìë¥¼ ìª¼ê·¸ë¼íŠ¸ë ¤ í•™ìŠµ)\")\n","\n","    # ë°ì´í„° ë¡œë“œ\n","    train_main = DBDataset(os.path.join(BASE_PATH, 'images/train'), os.path.join(BASE_PATH, 'jsons/train.json'), transform=train_transform, mode='train')\n","    pseudo_datasets = []\n","    for folder in ['sroie', 'cord-v2', 'wildreceipt']:\n","        p_img = os.path.join(PSEUDO_PATH, folder, 'images')\n","        p_json = os.path.join(PSEUDO_PATH, folder, 'train.json')\n","        if os.path.exists(p_json):\n","            pseudo_datasets.append(DBDataset(p_img, p_json, transform=train_transform, mode='train'))\n","\n","    full_ds = ConcatDataset([train_main] + pseudo_datasets)\n","    train_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n","\n","    # ëª¨ë¸ ì •ì˜: FPN ì‚¬ìš© (DBNet í•„ìˆ˜)\n","    model = smp.FPN(\n","        encoder_name=ENCODER_NAME,\n","        encoder_weights=\"imagenet\",\n","        in_channels=3,\n","        classes=1,\n","    ).to(DEVICE)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(train_loader))\n","\n","    # Loss: ê²½ê³„ ë¶„ë¦¬ë¥¼ ìœ„í•´ Dice Loss ë¹„ì¤‘ ë†’ì„\n","    dice_loss = smp.losses.DiceLoss(mode='binary')\n","    bce_loss = smp.losses.SoftBCEWithLogitsLoss()\n","    scaler = GradScaler('cuda')\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        model.train()\n","        epoch_loss = 0\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n","\n","        for imgs, msks in pbar:\n","            imgs, msks = imgs.to(DEVICE), msks.to(DEVICE).unsqueeze(1)\n","            optimizer.zero_grad()\n","\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                preds = model(imgs)\n","                loss = dice_loss(preds, msks) + bce_loss(preds, msks)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            scheduler.step()\n","            epoch_loss += loss.item()\n","            pbar.set_postfix(loss=loss.item())\n","\n","    # ëª¨ë¸ ì €ì¥\n","    save_path = f\"{MODEL_NAME}_final.pth\"\n","    torch.save(model.state_dict(), save_path)\n","    print(f\"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨: {save_path}\")\n","    return save_path\n","\n","# ==========================================\n","# 4. ì¶”ë¡  ë° í›„ì²˜ë¦¬ í•¨ìˆ˜ (Unclip)\n","# ==========================================\n","def unclip(box, unclip_ratio):\n","    \"\"\"ìª¼ê·¸ë¼ë“  ì˜ì—­ì„ ë‹¤ì‹œ íŒ½ì°½ì‹œí‚¤ëŠ” DBNet í•µì‹¬ ë¡œì§\"\"\"\n","    poly = Polygon(box)\n","    area = poly.area\n","    length = poly.length\n","    if area <= 0 or length <= 0: return box\n","\n","    # íŒ½ì°½ì‹œí‚¬ ê±°ë¦¬ ê³„ì‚°\n","    distance = area * unclip_ratio / length\n","\n","    offset = pyclipper.PyclipperOffset()\n","    offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n","    expanded = offset.Execute(distance) # í”ŒëŸ¬ìŠ¤ ê±°ë¦¬ = íŒ½ì°½\n","\n","    return expanded\n","\n","def inference(model_path):\n","    print(f\"ğŸš€ ì¶”ë¡  ë° íŒŒì¼ ìƒì„± ì‹œì‘... (Unclip Ratio: {UNCLIP_RATIO})\")\n","\n","    # ëª¨ë¸ ë¡œë“œ\n","    model = smp.FPN(encoder_name=ENCODER_NAME, in_channels=3, classes=1).to(DEVICE)\n","    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n","    model.eval()\n","\n","    # ë°ì´í„° ë¡œë“œ\n","    test_ds = DBDataset(os.path.join(BASE_PATH, 'images/test'), os.path.join(BASE_PATH, 'jsons/test.json'), transform=test_transform, mode='test')\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","\n","    results = {}\n","\n","    with torch.no_grad():\n","        for imgs, names, (orig_hs, orig_ws) in tqdm(test_loader, desc=\"Inference\"):\n","            imgs = imgs.to(DEVICE)\n","\n","            # TTA: ì¢Œìš° ë°˜ì „ ì•™ìƒë¸”\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                p1 = torch.sigmoid(model(imgs))\n","                p2 = torch.sigmoid(model(torch.flip(imgs, dims=[3])))\n","                preds = (p1 + torch.flip(p2, dims=[3])) / 2\n","\n","            preds = preds.float().cpu().numpy()\n","\n","            for i, name in enumerate(names):\n","                orig_h = orig_hs[i].item()\n","                orig_w = orig_ws[i].item()\n","                pred_mask = preds[i][0]\n","\n","                # 1. ì›ë³¸ í¬ê¸° ë³µì›\n","                pred_mask = cv2.resize(pred_mask, (orig_w, orig_h))\n","\n","                # 2. ì´ì§„í™”\n","                binary_mask = (pred_mask > BOX_THRESH).astype(np.uint8)\n","\n","                # 3. ìœ¤ê³½ì„  ê²€ì¶œ\n","                contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","                polygons = []\n","                for cnt in contours:\n","                    if cv2.contourArea(cnt) < 50: continue # ë…¸ì´ì¦ˆ ì œê±°\n","\n","                    epsilon = 0.005 * cv2.arcLength(cnt, True)\n","                    approx = cv2.approxPolyDP(cnt, epsilon, True)\n","\n","                    if len(approx) < 3: continue\n","\n","                    reshaped_poly = approx.reshape(-1, 2)\n","\n","                    # 4. [DBNet Post-processing] Unclip (íŒ½ì°½)\n","                    # í•™ìŠµ ë•Œ ì¤„ì–´ë“  ë§Œí¼, í˜¹ì€ ê·¸ ì´ìƒìœ¼ë¡œ í‚¤ì›Œì„œ Recall í™•ë³´ + Precision ìœ ì§€\n","                    try:\n","                        expanded = unclip(reshaped_poly, UNCLIP_RATIO)\n","                        if len(expanded) > 0:\n","                            final_poly = np.array(expanded[0])\n","                            polygons.append(final_poly.reshape(-1).tolist())\n","                        else:\n","                            polygons.append(reshaped_poly.reshape(-1).tolist())\n","                    except:\n","                        polygons.append(reshaped_poly.reshape(-1).tolist())\n","\n","                results[name] = \"|\".join([\" \".join(map(str, poly)) for poly in polygons])\n","\n","    # íŒŒì¼ ìƒì„±\n","    sample = pd.read_csv(os.path.join(BASE_PATH, 'sample_submission.csv'))\n","    sample['polygons'] = sample['filename'].map(results).fillna(\"\")\n","    sample.to_csv(OUTPUT_CSV, index=False)\n","    print(f\"ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì œì¶œ íŒŒì¼: {OUTPUT_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    # 1. í•™ìŠµ ì‹¤í–‰\n","    saved_model = train()\n","\n","    # 2. ë°”ë¡œ ì¶”ë¡  ì‹¤í–‰\n","    inference(saved_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":603,"referenced_widgets":["50a99d7ad52549c79d7d30e53eec1066","b1f42c4e543f4358bd5545e084f71ec6","58604b9f795748dd9c04d0a9be073c6f","08b7ad6244eb49dca00578b22fff22ad","69c5c4131eb344f88cc3143dbf4c32c4","1536782fee2e4ae99ee5093db79cba49","f85ffc9607e24a82aebbef7d3ee9d797","0dc494ab17f8407891d618ad84231251","0f9c316775a640ce9c6a3e4baecab68f","aac64bb2a78943e3bdfca0108801f5c5","4b0676d8b88943c9b123bc06d2a4df05"]},"id":"6OcyEF5mkVt5","executionInfo":{"status":"ok","timestamp":1770378289201,"user_tz":-540,"elapsed":3249674,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"bf439d32-53f3-459a-9897-7c753c209984"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”¥ DBNet í•™ìŠµ ì‹œì‘! (Encoder: tu-convnext_base)\n","   - Shrink Ratio: 0.4 (ê¸€ìë¥¼ ìª¼ê·¸ë¼íŠ¸ë ¤ í•™ìŠµ)\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50a99d7ad52549c79d7d30e53eec1066"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Epoch 1/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:23<00:00,  2.85it/s, loss=0.334]\n","Epoch 2/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.303]\n","Epoch 3/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.264]\n","Epoch 4/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.288]\n","Epoch 5/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.261]\n","Epoch 6/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.275]\n","Epoch 7/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:08<00:00,  3.19it/s, loss=0.277]\n","Epoch 8/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.312]\n","Epoch 9/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.261]\n","Epoch 10/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:08<00:00,  3.19it/s, loss=0.248]\n","Epoch 11/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.28]\n","Epoch 12/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:08<00:00,  3.19it/s, loss=0.236]\n","Epoch 13/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.24]\n","Epoch 14/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.226]\n","Epoch 15/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.242]\n","Epoch 16/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.228]\n","Epoch 17/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:08<00:00,  3.19it/s, loss=0.228]\n","Epoch 18/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:08<00:00,  3.20it/s, loss=0.199]\n","Epoch 19/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.217]\n","Epoch 20/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.239]\n","Epoch 21/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.234]\n","Epoch 22/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.207]\n","Epoch 23/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:07<00:00,  3.20it/s, loss=0.228]\n","Epoch 24/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:09<00:00,  3.17it/s, loss=0.231]\n","Epoch 25/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409/409 [02:08<00:00,  3.19it/s, loss=0.223]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨: DBNet_ConvNeXt_FPN_final.pth\n","ğŸš€ ì¶”ë¡  ë° íŒŒì¼ ìƒì„± ì‹œì‘... (Unclip Ratio: 1.8)\n"]},{"output_type":"stream","name":"stderr","text":["Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:21<00:00,  2.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! ì œì¶œ íŒŒì¼: submission_dbnet_final.csv\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import segmentation_models_pytorch as smp\n","import pyclipper\n","from shapely.geometry import Polygon\n","from tqdm import tqdm\n","import ssl\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# --- [íŠœë‹ í•µì‹¬] Recall ê·¹ëŒ€í™” ì„¤ì • ---\n","ENCODER_NAME = \"tu-convnext_base\"\n","# ê¸°ì¡´ì— í•™ìŠµ ì™„ë£Œí•œ ëª¨ë¸ ê²½ë¡œ (íŒŒì¼ëª… í™•ì¸ í•„ìš”!)\n","MODEL_PATH = \"DBNet_ConvNeXt_FPN_final.pth\"\n","OUTPUT_CSV = \"submission_dbnet_high_recall.csv\"\n","\n","# ğŸš¨ [ì—¬ê¸°ë¥¼ ì£¼ëª©í•˜ì„¸ìš”] ğŸš¨\n","# Precision 0.98ì„ ë¯¿ê³  ì•„ì£¼ ê³¼ê°í•˜ê²Œ ì„¸íŒ…í•©ë‹ˆë‹¤.\n","UNCLIP_RATIO = 3.0  # (ê¸°ì¡´ 1.8 -> 3.0) ìª¼ê·¸ë¼ë“  ì˜ì—­ì„ 3ë°°ë¡œ í™• ë¶ˆë¦½ë‹ˆë‹¤!\n","BOX_THRESH = 0.3    # (ê¸°ì¡´ 0.5 -> 0.3) í¬ë¯¸í•œ ê¸€ìë„ 30% í™•ì‹ ë§Œ ìˆìœ¼ë©´ ì¡ìŠµë‹ˆë‹¤.\n","\n","BASE_PATH = './data/datasets'\n","TEST_IMG_DIR = os.path.join(BASE_PATH, 'images/test')\n","TEST_JSON = os.path.join(BASE_PATH, 'jsons/test.json')\n","SAMPLE_SUB = os.path.join(BASE_PATH, 'sample_submission.csv')\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","IMG_SIZE = 1024\n","\n","class TestDataset(Dataset):\n","    def __init__(self, img_dir, json_path, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            self.data = json.load(f)['images']\n","        self.image_names = list(self.data.keys())\n","\n","    def __len__(self): return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        name = self.image_names[idx]\n","        image = cv2.imread(os.path.join(self.img_dir, name))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        orig_h, orig_w = image.shape[:2]\n","        if self.transform:\n","            image = self.transform(image=image)['image']\n","        return image, name, (orig_h, orig_w)\n","\n","def unclip(box, unclip_ratio):\n","    poly = Polygon(box)\n","    area = poly.area\n","    length = poly.length\n","    if area <= 0 or length <= 0: return box\n","\n","    # ê±°ë¦¬ ê³„ì‚°: ë©´ì  ëŒ€ë¹„ ë‘˜ë ˆ ë¹„ìœ¨ë¡œ í™•ì¥ ê±°ë¦¬ ê²°ì •\n","    distance = area * unclip_ratio / length\n","\n","    offset = pyclipper.PyclipperOffset()\n","    offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n","    expanded = offset.Execute(distance)\n","\n","    return expanded\n","\n","def preds_to_polygons(pred_mask, orig_h, orig_w):\n","    # 1. ì›ë³¸ í¬ê¸° ë³µì›\n","    pred_mask = cv2.resize(pred_mask, (orig_w, orig_h))\n","\n","    # 2. ì´ì§„í™” (ë‚®ì•„ì§„ Threshold ì ìš©)\n","    binary_mask = (pred_mask > BOX_THRESH).astype(np.uint8)\n","\n","    # 3. ìœ¤ê³½ì„  ê²€ì¶œ\n","    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    polygons = []\n","    for cnt in contours:\n","        if cv2.contourArea(cnt) < 30: continue # ì•„ì£¼ ì‘ì€ ë…¸ì´ì¦ˆë§Œ ì œê±°\n","\n","        epsilon = 0.003 * cv2.arcLength(cnt, True) # ë” ì •êµí•˜ê²Œ (0.005 -> 0.003)\n","        approx = cv2.approxPolyDP(cnt, epsilon, True)\n","\n","        if len(approx) < 3: continue\n","\n","        reshaped_poly = approx.reshape(-1, 2)\n","\n","        # 4. Unclip (í™•ì¥) ì ìš©\n","        try:\n","            expanded = unclip(reshaped_poly, UNCLIP_RATIO)\n","            if len(expanded) > 0:\n","                final_poly = np.array(expanded[0])\n","                polygons.append(final_poly.reshape(-1).tolist())\n","            else:\n","                polygons.append(reshaped_poly.reshape(-1).tolist())\n","        except:\n","            polygons.append(reshaped_poly.reshape(-1).tolist())\n","\n","    return \"|\".join([\" \".join(map(str, poly)) for poly in polygons])\n","\n","def main():\n","    print(f\"ğŸ”¥ Recall ë¶€ìŠ¤íŒ… ì‹œì‘! (Unclip: {UNCLIP_RATIO}, Thresh: {BOX_THRESH})\")\n","\n","    model = smp.FPN(\n","        encoder_name=ENCODER_NAME,\n","        encoder_weights=None,\n","        in_channels=3,\n","        classes=1,\n","    ).to(DEVICE)\n","\n","    if not os.path.exists(MODEL_PATH):\n","        print(f\"âŒ ëª¨ë¸ íŒŒì¼({MODEL_PATH})ì´ ì—†ìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","    model.eval()\n","\n","    test_transform = A.Compose([\n","        A.Resize(IMG_SIZE, IMG_SIZE),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ])\n","\n","    test_ds = TestDataset(TEST_IMG_DIR, TEST_JSON, transform=test_transform)\n","    test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=4)\n","\n","    results = {}\n","\n","    with torch.no_grad():\n","        for imgs, names, (orig_hs, orig_ws) in tqdm(test_loader):\n","            imgs = imgs.to(DEVICE)\n","\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                p1 = torch.sigmoid(model(imgs))\n","                p2 = torch.sigmoid(model(torch.flip(imgs, dims=[3])))\n","                preds = (p1 + torch.flip(p2, dims=[3])) / 2\n","\n","            preds = preds.float().cpu().numpy()\n","\n","            for i, name in enumerate(names):\n","                orig_h = orig_hs[i].item()\n","                orig_w = orig_ws[i].item()\n","\n","                poly_str = preds_to_polygons(preds[i][0], orig_h, orig_w)\n","                results[name] = poly_str\n","\n","    sample_df = pd.read_csv(SAMPLE_SUB)\n","    sample_df['polygons'] = sample_df['filename'].map(results).fillna(\"\")\n","    sample_df.to_csv(OUTPUT_CSV, index=False)\n","    print(f\"ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: {OUTPUT_CSV}\")\n","    print(\"ğŸ‘‰ ì´ íŒŒì¼ì€ 'ë„ˆë¬´ ë†’ì•˜ë˜ Precision'ì„ ì¡°ê¸ˆ í¬ìƒí•˜ê³  Recallì„ ëŒ€í­ ê°€ì ¸ì˜µë‹ˆë‹¤.\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"104DLgUykph7","executionInfo":{"status":"ok","timestamp":1770378703389,"user_tz":-540,"elapsed":21882,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"97482146-93a2-4fad-cbf5-7c7441c62709"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”¥ Recall ë¶€ìŠ¤íŒ… ì‹œì‘! (Unclip: 3.0, Thresh: 0.3)\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:19<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ‰ íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_dbnet_high_recall.csv\n","ğŸ‘‰ ì´ íŒŒì¼ì€ 'ë„ˆë¬´ ë†’ì•˜ë˜ Precision'ì„ ì¡°ê¸ˆ í¬ìƒí•˜ê³  Recallì„ ëŒ€í­ ê°€ì ¸ì˜µë‹ˆë‹¤.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yRujtOVwytvZ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.0"},"colab":{"provenance":[{"file_id":"1QvfUG_AbzI3jDiZKueskEYBJ9R8UezHJ","timestamp":1770374775092}],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"50a99d7ad52549c79d7d30e53eec1066":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1f42c4e543f4358bd5545e084f71ec6","IPY_MODEL_58604b9f795748dd9c04d0a9be073c6f","IPY_MODEL_08b7ad6244eb49dca00578b22fff22ad"],"layout":"IPY_MODEL_69c5c4131eb344f88cc3143dbf4c32c4"}},"b1f42c4e543f4358bd5545e084f71ec6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1536782fee2e4ae99ee5093db79cba49","placeholder":"â€‹","style":"IPY_MODEL_f85ffc9607e24a82aebbef7d3ee9d797","value":"model.safetensors:â€‡100%"}},"58604b9f795748dd9c04d0a9be073c6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dc494ab17f8407891d618ad84231251","max":354400320,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f9c316775a640ce9c6a3e4baecab68f","value":354400320}},"08b7ad6244eb49dca00578b22fff22ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aac64bb2a78943e3bdfca0108801f5c5","placeholder":"â€‹","style":"IPY_MODEL_4b0676d8b88943c9b123bc06d2a4df05","value":"â€‡354M/354Mâ€‡[00:03&lt;00:00,â€‡293MB/s]"}},"69c5c4131eb344f88cc3143dbf4c32c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1536782fee2e4ae99ee5093db79cba49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f85ffc9607e24a82aebbef7d3ee9d797":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dc494ab17f8407891d618ad84231251":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f9c316775a640ce9c6a3e4baecab68f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aac64bb2a78943e3bdfca0108801f5c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b0676d8b88943c9b123bc06d2a4df05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}