{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Precision OCR Model (Target: 95+ Score)\n",
    "\n",
    "## Key Improvements:\n",
    "1. **Stronger Backbone**: EfficientNet-b5 / ConvNeXt\n",
    "2. **Higher Resolution**: 1536x1536\n",
    "3. **Improved Loss**: Focal + Dice + Boundary Loss\n",
    "4. **Multi-scale TTA**: Ensemble multiple scales\n",
    "5. **Refined Post-processing**: Morphological ops + Adaptive threshold\n",
    "6. **Pseudo Label**: Additional training data\n",
    "7. **Precision/Recall Metrics**: Track actual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Data Download\n",
    "!wget -O data.tar.gz \"https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\"\n",
    "!tar -xzf data.tar.gz\n",
    "!ls -la data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Dependencies\n",
    "!pip install -q segmentation-models-pytorch albumentations opencv-python-headless timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Imports and Configuration\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = './data/datasets'\n",
    "TRAIN_IMG_DIR = os.path.join(BASE_PATH, 'images/train')\n",
    "VAL_IMG_DIR = os.path.join(BASE_PATH, 'images/val')\n",
    "TEST_IMG_DIR = os.path.join(BASE_PATH, 'images/test')\n",
    "TRAIN_JSON = os.path.join(BASE_PATH, 'jsons/train.json')\n",
    "VAL_JSON = os.path.join(BASE_PATH, 'jsons/val.json')\n",
    "TEST_JSON = os.path.join(BASE_PATH, 'jsons/test.json')\n",
    "SAMPLE_SUB = os.path.join(BASE_PATH, 'sample_submission.csv')\n",
    "\n",
    "# Pseudo Label Paths\n",
    "PSEUDO_BASE = './data/pseudo_label'\n",
    "SROIE_TRAIN_IMG = os.path.join(PSEUDO_BASE, 'sroie/images/train')\n",
    "SROIE_TEST_IMG = os.path.join(PSEUDO_BASE, 'sroie/images/test')\n",
    "WILDRECEIPT_IMG = os.path.join(PSEUDO_BASE, 'wildreceipt/images')\n",
    "CORDV2_IMG = os.path.join(PSEUDO_BASE, 'cord-v2/images')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Hyperparameters (Optimized for High Precision)\n",
    "class Config:\n",
    "    # Model\n",
    "    ENCODER = 'tu-convnext_base'  # ConvNeXt for better feature extraction\n",
    "    ENCODER_WEIGHTS = 'imagenet'\n",
    "    \n",
    "    # Training\n",
    "    RESIZE_TARGET = 1536  # Higher resolution for small text\n",
    "    BATCH_SIZE = 2        # Reduced for high resolution\n",
    "    ACCUMULATION_STEPS = 16  # Effective batch size = 32\n",
    "    EPOCHS = 40\n",
    "    LEARNING_RATE = 5e-5  # Lower LR for stability\n",
    "    WARMUP_EPOCHS = 3\n",
    "    \n",
    "    # Loss weights\n",
    "    DICE_WEIGHT = 0.4\n",
    "    BCE_WEIGHT = 0.3\n",
    "    FOCAL_WEIGHT = 0.3\n",
    "    \n",
    "    # Inference\n",
    "    THRESHOLD = 0.5\n",
    "    MIN_AREA = 50  # Minimum polygon area\n",
    "    \n",
    "    # TTA scales\n",
    "    TTA_SCALES = [1.0, 0.75, 1.25]\n",
    "    \n",
    "    # Use pseudo labels\n",
    "    USE_PSEUDO_LABELS = True\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"Config: Resolution={cfg.RESIZE_TARGET}, Encoder={cfg.ENCODER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Advanced Data Augmentation\n",
    "def get_train_transform(size):\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size),\n",
    "        # Geometric transforms\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.3),\n",
    "        A.Perspective(scale=(0.02, 0.08), p=0.4),\n",
    "        A.Affine(\n",
    "            scale=(0.9, 1.1),\n",
    "            rotate=(-5, 5),\n",
    "            shear=(-5, 5),\n",
    "            p=0.3\n",
    "        ),\n",
    "        # Color transforms (for receipt images)\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8)),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
    "            A.RandomGamma(gamma_limit=(80, 120)),\n",
    "        ], p=0.5),\n",
    "        # Noise and blur\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=(3, 5)),\n",
    "            A.MotionBlur(blur_limit=3),\n",
    "            A.MedianBlur(blur_limit=3),\n",
    "        ], p=0.2),\n",
    "        A.GaussNoise(std_range=(0.02, 0.1), p=0.2),\n",
    "        # Normalize\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_val_transform(size):\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "print(\"Transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset Classes\n",
    "class ReceiptDataset(Dataset):\n",
    "    \"\"\"Main dataset for train/val with JSON labels\"\"\"\n",
    "    def __init__(self, img_dir, json_path, transform=None, is_test=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)['images']\n",
    "        self.image_names = list(self.data.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        if not self.is_test:\n",
    "            # Create mask from polygon annotations\n",
    "            mask = np.zeros((h, w), dtype=np.float32)\n",
    "            words = self.data[img_name].get('words', {})\n",
    "            for word_id, word_info in words.items():\n",
    "                points = np.array(word_info['points'], dtype=np.int32)\n",
    "                cv2.fillPoly(mask, [points], 1.0)\n",
    "            \n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image, mask=mask)\n",
    "                return augmented['image'], augmented['mask']\n",
    "            return image, mask\n",
    "        else:\n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=image)\n",
    "                return augmented['image'], img_name, (h, w)\n",
    "            return image, img_name, (h, w)\n",
    "\n",
    "\n",
    "class PseudoLabelDataset(Dataset):\n",
    "    \"\"\"Dataset for pseudo-labeled images (use image only, generate mask from model)\"\"\"\n",
    "    def __init__(self, img_dirs, transform=None):\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        \n",
    "        for img_dir in img_dirs:\n",
    "            if os.path.exists(img_dir):\n",
    "                for fname in os.listdir(img_dir):\n",
    "                    if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.images.append(os.path.join(img_dir, fname))\n",
    "        \n",
    "        print(f\"Pseudo Label: Found {len(self.images)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            return augmented['image']\n",
    "        return image\n",
    "\n",
    "print(\"Dataset classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Advanced Loss Functions\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for imbalanced data\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred = pred.clamp(1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Focal loss computation\n",
    "        pt = torch.where(target == 1, pred, 1 - pred)\n",
    "        alpha_t = torch.where(target == 1, self.alpha, 1 - self.alpha)\n",
    "        focal_weight = alpha_t * (1 - pt) ** self.gamma\n",
    "        \n",
    "        bce = F.binary_cross_entropy(pred, target, reduction='none')\n",
    "        loss = focal_weight * bce\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"Boundary-aware loss for sharper edges\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Sobel kernels for edge detection\n",
    "        self.register_buffer('sobel_x', torch.tensor(\n",
    "            [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32\n",
    "        ).view(1, 1, 3, 3))\n",
    "        self.register_buffer('sobel_y', torch.tensor(\n",
    "            [[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32\n",
    "        ).view(1, 1, 3, 3))\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        \n",
    "        # Compute gradients for predicted mask\n",
    "        pred_gx = F.conv2d(pred, self.sobel_x, padding=1)\n",
    "        pred_gy = F.conv2d(pred, self.sobel_y, padding=1)\n",
    "        pred_edge = torch.sqrt(pred_gx ** 2 + pred_gy ** 2 + 1e-8)\n",
    "        \n",
    "        # Compute gradients for target mask\n",
    "        target = target.unsqueeze(1) if target.dim() == 3 else target\n",
    "        target_gx = F.conv2d(target, self.sobel_x, padding=1)\n",
    "        target_gy = F.conv2d(target, self.sobel_y, padding=1)\n",
    "        target_edge = torch.sqrt(target_gx ** 2 + target_gy ** 2 + 1e-8)\n",
    "        \n",
    "        # MSE between edges\n",
    "        return F.mse_loss(pred_edge, target_edge)\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined loss: Dice + BCE + Focal + Boundary\"\"\"\n",
    "    def __init__(self, dice_w=0.4, bce_w=0.3, focal_w=0.2, boundary_w=0.1):\n",
    "        super().__init__()\n",
    "        self.dice = smp.losses.DiceLoss(mode='binary')\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.focal = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        self.boundary = BoundaryLoss()\n",
    "        \n",
    "        self.dice_w = dice_w\n",
    "        self.bce_w = bce_w\n",
    "        self.focal_w = focal_w\n",
    "        self.boundary_w = boundary_w\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        target = target.unsqueeze(1) if target.dim() == 3 else target\n",
    "        \n",
    "        loss = (\n",
    "            self.dice_w * self.dice(pred, target) +\n",
    "            self.bce_w * self.bce(pred, target) +\n",
    "            self.focal_w * self.focal(pred, target) +\n",
    "            self.boundary_w * self.boundary(pred, target)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "print(\"Loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model Architecture with Attention\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Channel and Spatial Attention\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        # Channel attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 8, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // 8, in_channels, 1, bias=False)\n",
    "        )\n",
    "        # Spatial attention\n",
    "        self.conv = nn.Conv2d(2, 1, 7, padding=3, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        channel_att = torch.sigmoid(avg_out + max_out)\n",
    "        x = x * channel_att\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_att = torch.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))\n",
    "        x = x * spatial_att\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class HighPrecisionOCRModel(nn.Module):\n",
    "    \"\"\"UNet++ with Attention for High Precision OCR\"\"\"\n",
    "    def __init__(self, encoder_name, encoder_weights='imagenet'):\n",
    "        super().__init__()\n",
    "        self.base_model = smp.UnetPlusPlus(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            decoder_attention_type='scse'  # Spatial and Channel SE attention\n",
    "        )\n",
    "        \n",
    "        # Additional refinement head\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.base_model(x)\n",
    "        out = self.refine(out)\n",
    "        return out\n",
    "\n",
    "print(\"Model architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Precision/Recall Metrics\n",
    "def compute_iou(mask1, mask2):\n",
    "    \"\"\"Compute IoU between two binary masks\"\"\"\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def compute_precision_recall(pred_mask, gt_mask, threshold=0.5):\n",
    "    \"\"\"Compute pixel-level precision and recall\"\"\"\n",
    "    pred_binary = (pred_mask > threshold).astype(np.float32)\n",
    "    gt_binary = gt_mask.astype(np.float32)\n",
    "    \n",
    "    tp = np.logical_and(pred_binary, gt_binary).sum()\n",
    "    fp = np.logical_and(pred_binary, ~gt_binary.astype(bool)).sum()\n",
    "    fn = np.logical_and(~pred_binary.astype(bool), gt_binary).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device, threshold=0.5):\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.numpy()\n",
    "            \n",
    "            with autocast('cuda', dtype=torch.bfloat16):\n",
    "                preds = torch.sigmoid(model(images))\n",
    "            preds = preds.float().cpu().numpy()\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                pred = preds[i, 0]\n",
    "                gt = masks[i]\n",
    "                \n",
    "                p, r, f1 = compute_precision_recall(pred, gt, threshold)\n",
    "                iou = compute_iou(pred > threshold, gt > 0.5)\n",
    "                \n",
    "                total_precision += p\n",
    "                total_recall += r\n",
    "                total_f1 += f1\n",
    "                total_iou += iou\n",
    "                count += 1\n",
    "    \n",
    "    return {\n",
    "        'precision': total_precision / count,\n",
    "        'recall': total_recall / count,\n",
    "        'f1': total_f1 / count,\n",
    "        'iou': total_iou / count\n",
    "    }\n",
    "\n",
    "print(\"Metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Advanced Post-processing\n",
    "def apply_morphology(mask, kernel_size=3):\n",
    "    \"\"\"Apply morphological operations to clean up mask\"\"\"\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "    \n",
    "    # Close small gaps\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    # Remove small noise\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def mask_to_polygons_advanced(mask, min_area=50, epsilon_factor=0.003):\n",
    "    \"\"\"Convert mask to polygons with advanced filtering\"\"\"\n",
    "    # Apply morphology first\n",
    "    mask = apply_morphology((mask * 255).astype(np.uint8))\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area < min_area:\n",
    "            continue\n",
    "        \n",
    "        # Get bounding rect for aspect ratio check\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        \n",
    "        # Filter out too thin or too small polygons\n",
    "        if w < 5 or h < 3:\n",
    "            continue\n",
    "        \n",
    "        # Simplify contour\n",
    "        epsilon = epsilon_factor * cv2.arcLength(cnt, True)\n",
    "        approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "        \n",
    "        # Need at least 4 points for a valid polygon\n",
    "        if len(approx) >= 4:\n",
    "            # Convert to convex hull if needed for cleaner polygons\n",
    "            if len(approx) > 8:\n",
    "                hull = cv2.convexHull(approx)\n",
    "                approx = hull\n",
    "            \n",
    "            polygons.append(approx.reshape(-1, 2).tolist())\n",
    "    \n",
    "    return polygons\n",
    "\n",
    "\n",
    "def polygons_to_string(polygons):\n",
    "    \"\"\"Convert polygons to submission format string\"\"\"\n",
    "    if not polygons:\n",
    "        return \"\"\n",
    "    return \"|\".join([\n",
    "        \" \".join([f\"{int(p[0])} {int(p[1])}\" for p in poly])\n",
    "        for poly in polygons\n",
    "    ])\n",
    "\n",
    "print(\"Post-processing functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Multi-scale TTA Inference\n",
    "def multi_scale_tta_inference(model, image, device, scales=[1.0, 0.75, 1.25], original_size=None):\n",
    "    \"\"\"\n",
    "    Perform multi-scale TTA inference\n",
    "    Args:\n",
    "        model: trained model\n",
    "        image: input tensor (C, H, W)\n",
    "        device: torch device\n",
    "        scales: list of scale factors\n",
    "        original_size: (h, w) for resizing back\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    _, h, w = image.shape\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for scale in scales:\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            # Ensure divisible by 32\n",
    "            new_h = (new_h // 32) * 32\n",
    "            new_w = (new_w // 32) * 32\n",
    "            if new_h == 0:\n",
    "                new_h = 32\n",
    "            if new_w == 0:\n",
    "                new_w = 32\n",
    "            \n",
    "            # Resize image\n",
    "            scaled_img = F.interpolate(\n",
    "                image.unsqueeze(0), size=(new_h, new_w), mode='bilinear', align_corners=False\n",
    "            ).to(device)\n",
    "            \n",
    "            # Original prediction\n",
    "            with autocast('cuda', dtype=torch.bfloat16):\n",
    "                pred = torch.sigmoid(model(scaled_img))\n",
    "            pred = F.interpolate(pred, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            all_preds.append(pred.float())\n",
    "            \n",
    "            # Horizontal flip\n",
    "            with autocast('cuda', dtype=torch.bfloat16):\n",
    "                pred_flip = torch.sigmoid(model(torch.flip(scaled_img, dims=[3])))\n",
    "            pred_flip = torch.flip(pred_flip, dims=[3])\n",
    "            pred_flip = F.interpolate(pred_flip, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            all_preds.append(pred_flip.float())\n",
    "    \n",
    "    # Average all predictions\n",
    "    final_pred = torch.stack(all_preds, dim=0).mean(dim=0)\n",
    "    \n",
    "    # Resize to original size if provided\n",
    "    if original_size is not None:\n",
    "        final_pred = F.interpolate(\n",
    "            final_pred, size=original_size, mode='bilinear', align_corners=False\n",
    "        )\n",
    "    \n",
    "    return final_pred.cpu().numpy()[0, 0]\n",
    "\n",
    "print(\"Multi-scale TTA inference defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Training Function\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, accumulation_steps):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for i, batch in enumerate(pbar):\n",
    "        images, masks = batch[0], batch[1]\n",
    "        images = images.to(device, non_blocking=True, memory_format=torch.channels_last)\n",
    "        masks = masks.to(device, non_blocking=True).unsqueeze(1)\n",
    "        \n",
    "        with autocast('cuda', dtype=torch.bfloat16):\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, masks) / accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        pbar.set_postfix({'loss': f'{total_loss / (i + 1):.4f}'})\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.to(device, memory_format=torch.channels_last)\n",
    "            masks = masks.to(device).unsqueeze(1)\n",
    "            \n",
    "            with autocast('cuda', dtype=torch.bfloat16):\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Main Training Pipeline\n",
    "def main_training():\n",
    "    print(\"=\"*60)\n",
    "    print(\"High Precision OCR Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Prepare datasets\n",
    "    print(\"\\n[1/5] Preparing datasets...\")\n",
    "    train_transform = get_train_transform(cfg.RESIZE_TARGET)\n",
    "    val_transform = get_val_transform(cfg.RESIZE_TARGET)\n",
    "    \n",
    "    train_ds = ReceiptDataset(TRAIN_IMG_DIR, TRAIN_JSON, transform=train_transform)\n",
    "    val_ds = ReceiptDataset(VAL_IMG_DIR, VAL_JSON, transform=val_transform)\n",
    "    \n",
    "    print(f\"  Train: {len(train_ds)} images\")\n",
    "    print(f\"  Val: {len(val_ds)} images\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True,\n",
    "        num_workers=4, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"\\n[2/5] Initializing model...\")\n",
    "    model = HighPrecisionOCRModel(\n",
    "        encoder_name=cfg.ENCODER,\n",
    "        encoder_weights=cfg.ENCODER_WEIGHTS\n",
    "    ).to(DEVICE)\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Model: {cfg.ENCODER}\")\n",
    "    print(f\"  Parameters: {total_params / 1e6:.1f}M\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    print(\"\\n[3/5] Setting up training...\")\n",
    "    criterion = CombinedLoss(\n",
    "        dice_w=cfg.DICE_WEIGHT,\n",
    "        bce_w=cfg.BCE_WEIGHT,\n",
    "        focal_w=cfg.FOCAL_WEIGHT,\n",
    "        boundary_w=0.1\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.LEARNING_RATE,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Scheduler: Warmup + Cosine\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=0.1, total_iters=cfg.WARMUP_EPOCHS\n",
    "    )\n",
    "    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=cfg.EPOCHS - cfg.WARMUP_EPOCHS, eta_min=1e-7\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer,\n",
    "        schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "        milestones=[cfg.WARMUP_EPOCHS]\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler('cuda')\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\n[4/5] Training...\")\n",
    "    best_loss = float('inf')\n",
    "    best_f1 = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(1, cfg.EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch {epoch}/{cfg.EPOCHS} ---\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler,\n",
    "            DEVICE, cfg.ACCUMULATION_STEPS\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = validate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Evaluate metrics every 5 epochs\n",
    "        if epoch % 5 == 0 or epoch == cfg.EPOCHS:\n",
    "            metrics = evaluate_model(model, val_loader, DEVICE, cfg.THRESHOLD)\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Precision: {metrics['precision']:.4f} | Recall: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f} | IoU: {metrics['iou']:.4f}\")\n",
    "            \n",
    "            if metrics['f1'] > best_f1:\n",
    "                best_f1 = metrics['f1']\n",
    "                torch.save(model.state_dict(), 'best_f1_model.pth')\n",
    "                print(\"New best F1 model saved!\")\n",
    "        else:\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best loss model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_loss_model.pth')\n",
    "            print(\"Best loss model saved!\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n[5/5] Training completed!\")\n",
    "    print(f\"Best Val Loss: {best_loss:.4f}\")\n",
    "    print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run training\n",
    "model = main_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Inference with Multi-scale TTA\n",
    "def run_inference(model_path='best_f1_model.pth', output_csv='submission_high_precision.csv'):\n",
    "    print(\"=\"*60)\n",
    "    print(\"High Precision Inference (Multi-scale TTA)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"\\n[1/3] Loading model...\")\n",
    "    model = HighPrecisionOCRModel(\n",
    "        encoder_name=cfg.ENCODER,\n",
    "        encoder_weights=None\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    print(f\"  Loaded: {model_path}\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(\"\\n[2/3] Running inference...\")\n",
    "    test_transform = get_val_transform(cfg.RESIZE_TARGET)\n",
    "    \n",
    "    with open(TEST_JSON, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)['images']\n",
    "    test_images = list(test_data.keys())\n",
    "    \n",
    "    preds = {}\n",
    "    \n",
    "    for img_name in tqdm(test_images, desc=\"Inference\"):\n",
    "        img_path = os.path.join(TEST_IMG_DIR, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            preds[img_name] = \"\"\n",
    "            continue\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "        \n",
    "        # Transform\n",
    "        transformed = test_transform(image=image)\n",
    "        img_tensor = transformed['image']\n",
    "        \n",
    "        # Multi-scale TTA\n",
    "        mask = multi_scale_tta_inference(\n",
    "            model, img_tensor, DEVICE,\n",
    "            scales=cfg.TTA_SCALES,\n",
    "            original_size=(orig_h, orig_w)\n",
    "        )\n",
    "        \n",
    "        # Post-processing\n",
    "        polygons = mask_to_polygons_advanced(\n",
    "            mask > cfg.THRESHOLD,\n",
    "            min_area=cfg.MIN_AREA,\n",
    "            epsilon_factor=0.003\n",
    "        )\n",
    "        \n",
    "        preds[img_name] = polygons_to_string(polygons)\n",
    "    \n",
    "    # Save submission\n",
    "    print(\"\\n[3/3] Saving submission...\")\n",
    "    sample_df = pd.read_csv(SAMPLE_SUB)\n",
    "    sample_df['polygons'] = sample_df['filename'].map(preds).fillna(\"\")\n",
    "    sample_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Statistics\n",
    "    polygon_counts = sample_df['polygons'].apply(lambda x: len(x.split('|')) if x else 0)\n",
    "    print(f\"\\nSubmission saved: {output_csv}\")\n",
    "    print(f\"  Total images: {len(sample_df)}\")\n",
    "    print(f\"  Avg polygons/image: {polygon_counts.mean():.1f}\")\n",
    "    print(f\"  Max polygons: {polygon_counts.max()}\")\n",
    "    print(f\"  Min polygons: {polygon_counts.min()}\")\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Run inference\n",
    "submission_df = run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Optional - Threshold Optimization\n",
    "def optimize_threshold(model, val_loader, device, thresholds=[0.3, 0.4, 0.5, 0.6, 0.7]):\n",
    "    \"\"\"Find optimal threshold for best F1 score\"\"\"\n",
    "    print(\"Optimizing threshold...\")\n",
    "    \n",
    "    best_thresh = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        metrics = evaluate_model(model, val_loader, device, threshold=thresh)\n",
    "        print(f\"  Threshold {thresh:.1f}: P={metrics['precision']:.4f}, R={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "        \n",
    "        if metrics['f1'] > best_f1:\n",
    "            best_f1 = metrics['f1']\n",
    "            best_thresh = thresh\n",
    "    \n",
    "    print(f\"\\nOptimal threshold: {best_thresh} (F1={best_f1:.4f})\")\n",
    "    return best_thresh\n",
    "\n",
    "# Uncomment to run threshold optimization\n",
    "# val_transform = get_val_transform(cfg.RESIZE_TARGET)\n",
    "# val_ds = ReceiptDataset(VAL_IMG_DIR, VAL_JSON, transform=val_transform)\n",
    "# val_loader = DataLoader(val_ds, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "# optimal_thresh = optimize_threshold(model, val_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Final Summary\n",
    "print(\"=\"*60)\n",
    "print(\"HIGH PRECISION OCR - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Key Improvements Applied:\n",
    "-------------------------\n",
    "1. Model: UNet++ with {cfg.ENCODER} backbone\n",
    "2. Resolution: {cfg.RESIZE_TARGET}x{cfg.RESIZE_TARGET} (high res for small text)\n",
    "3. Loss: Dice({cfg.DICE_WEIGHT}) + BCE({cfg.BCE_WEIGHT}) + Focal({cfg.FOCAL_WEIGHT}) + Boundary(0.1)\n",
    "4. Augmentation: Perspective, CLAHE, Blur, Noise\n",
    "5. TTA: Multi-scale ({cfg.TTA_SCALES}) + Horizontal Flip\n",
    "6. Post-processing: Morphological ops + Adaptive polygon extraction\n",
    "7. Training: Warmup + Cosine scheduler, gradient clipping\n",
    "\n",
    "Expected Improvements:\n",
    "----------------------\n",
    "- Higher precision through Focal Loss (reduces FP)\n",
    "- Better small text detection with high resolution\n",
    "- Sharper boundaries with Boundary Loss\n",
    "- Robust predictions with Multi-scale TTA\n",
    "- Cleaner polygons with morphological post-processing\n",
    "\n",
    "Files Generated:\n",
    "----------------\n",
    "- best_loss_model.pth: Best validation loss model\n",
    "- best_f1_model.pth: Best F1 score model\n",
    "- submission_high_precision.csv: Final submission\n",
    "\"\"\")\n",
    "print(\"Good luck with your submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
