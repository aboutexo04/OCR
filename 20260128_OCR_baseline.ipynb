{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyP0ZLX/ldnLOb9lOfj9welM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f2b3c301d7dd4eecad149bf050d38525":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26b31214c0df448cabf47f6e2f4df190","IPY_MODEL_ed518be5c0cd4c3e9b7de5dff825a411","IPY_MODEL_b772f5f18ea24f098a1e72d63f6f53a6"],"layout":"IPY_MODEL_22a27694a7bd4f22bb87ad8aab988d55"}},"26b31214c0df448cabf47f6e2f4df190":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a9d6e1e1564f0a98b1396003781550","placeholder":"â€‹","style":"IPY_MODEL_7924213a774a4f7ca093ea3db6281a65","value":"config.json:â€‡100%"}},"ed518be5c0cd4c3e9b7de5dff825a411":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b3268cb60d04b46a385368dad24a6ca","max":156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c896f1ab4f24c7b9892551a22e6c260","value":156}},"b772f5f18ea24f098a1e72d63f6f53a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_521ddf52a391496e8d9d7d36f9ffc1b1","placeholder":"â€‹","style":"IPY_MODEL_db6c494189274e508bb6bdb7cdffe622","value":"â€‡156/156â€‡[00:00&lt;00:00,â€‡19.5kB/s]"}},"22a27694a7bd4f22bb87ad8aab988d55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73a9d6e1e1564f0a98b1396003781550":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7924213a774a4f7ca093ea3db6281a65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b3268cb60d04b46a385368dad24a6ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c896f1ab4f24c7b9892551a22e6c260":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"521ddf52a391496e8d9d7d36f9ffc1b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db6c494189274e508bb6bdb7cdffe622":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c935120d07bf4223b255001e0faae625":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6756c59710704765ba7dd338a9d80352","IPY_MODEL_afa7eb5e4ed049609dad51bb3fc24999","IPY_MODEL_21c493a0cf094b14a14337b197b1557b"],"layout":"IPY_MODEL_8fa10f68a72f44958e6a63c12d4863aa"}},"6756c59710704765ba7dd338a9d80352":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1678f2a396464df59ae4b9863ab84361","placeholder":"â€‹","style":"IPY_MODEL_33c6305305b14661ad5dfd29e1a56617","value":"model.safetensors:â€‡100%"}},"afa7eb5e4ed049609dad51bb3fc24999":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72c781bc1b7a4f50b5da3a7f4262ff06","max":87275112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6f12444c4084d0092e0b34861290cc9","value":87275112}},"21c493a0cf094b14a14337b197b1557b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d8b86ac5f1d449a8e2f17ef08295f4b","placeholder":"â€‹","style":"IPY_MODEL_ba6654d5f475420d83b80a6a9b5b9fbf","value":"â€‡87.3M/87.3Mâ€‡[00:03&lt;00:00,â€‡34.1MB/s]"}},"8fa10f68a72f44958e6a63c12d4863aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1678f2a396464df59ae4b9863ab84361":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33c6305305b14661ad5dfd29e1a56617":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72c781bc1b7a4f50b5da3a7f4262ff06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6f12444c4084d0092e0b34861290cc9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d8b86ac5f1d449a8e2f17ef08295f4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba6654d5f475420d83b80a6a9b5b9fbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiSXFWwGgLfJ","executionInfo":{"status":"ok","timestamp":1769586484430,"user_tz":-540,"elapsed":110283,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"c93d15da-fa91-4855-f188-a59b57951877"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-01-28 07:46:14--  https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\n","Resolving aistages-api-public-prod.s3.amazonaws.com (aistages-api-public-prod.s3.amazonaws.com)... 3.5.187.45, 52.219.206.87, 3.5.189.87, ...\n","Connecting to aistages-api-public-prod.s3.amazonaws.com (aistages-api-public-prod.s3.amazonaws.com)|3.5.187.45|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3280795723 (3.1G) [binary/octet-stream]\n","Saving to: â€˜data.tar.gzâ€™\n","\n","data.tar.gz         100%[===================>]   3.05G  43.3MB/s    in 73s     \n","\n","2026-01-28 07:47:27 (42.7 MB/s) - â€˜data.tar.gzâ€™ saved [3280795723/3280795723]\n","\n","total 16\n","drwxr-xr-x 4 root root 4096 Dec 17  2024 .\n","drwxr-xr-x 1 root root 4096 Jan 28 07:47 ..\n","drwxr-xr-x 4 root root 4096 Jan 27  2024 datasets\n","drwxr-xr-x 5 root root 4096 Dec 17  2024 pseudo_label\n"]}],"source":["# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ & ì••ì¶• í•´ì œ\n","!wget -O data.tar.gz \"https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\"\n","!tar -xzf data.tar.gz\n","\n","# 2. íŒŒì¼ í™•ì¸\n","!ls -la data/"]},{"cell_type":"code","source":["!pip install albumentations segmentation-models-pytorch tqdm opencv-python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIQclPq7hoPE","executionInfo":{"status":"ok","timestamp":1769586495443,"user_tz":-540,"elapsed":4946,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"845b9ec7-9727-4afb-a77c-3a143199df22"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n","Collecting segmentation-models-pytorch\n","  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n","Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n","Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.12.3)\n","Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n","Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n","Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.6.0)\n","Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.12)\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.36.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.7.0)\n","Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.24)\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.9.0+cu126)\n","Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.24.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2026.1.4)\n","Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n","Successfully installed segmentation-models-pytorch-0.5.0\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast, GradScaler\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import segmentation_models_pytorch as smp\n","from tqdm import tqdm\n","import gc\n","\n","# --- 1. A100 ìµœì í™” ì„¤ì • ---\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","\n","# --- 2. ê²½ë¡œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n","BASE_PATH = './data/datasets'\n","TRAIN_IMG_DIR = os.path.join(BASE_PATH, 'images/train')\n","VAL_IMG_DIR = os.path.join(BASE_PATH, 'images/val')\n","TEST_IMG_DIR = os.path.join(BASE_PATH, 'images/test')\n","\n","TRAIN_JSON = os.path.join(BASE_PATH, 'jsons/train.json')\n","VAL_JSON = os.path.join(BASE_PATH, 'jsons/val.json')\n","TEST_JSON = os.path.join(BASE_PATH, 'jsons/test.json')\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# [A100 ìµœì í™” - ì•ˆì •ì ì¸ ì„¤ì •]\n","BATCH_SIZE = 16\n","ACCUMULATION_STEPS = 2      # ì‹¤íš¨ ë°°ì¹˜ = 32\n","RESIZE_TARGET = 512         # 768ì€ ë©”ëª¨ë¦¬ ë§ì´ ë¨¹ìŒ\n","EPOCHS = 20\n","LEARNING_RATE = 1e-4\n","\n","# --- 3. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ---\n","class ReceiptDataset(Dataset):\n","    def __init__(self, img_dir, json_path, transform=None, is_test=False):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.is_test = is_test\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            self.data = json.load(f)['images']\n","        self.image_names = list(self.data.keys())\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.img_dir, img_name)\n","        image = cv2.imread(img_path)\n","        if image is None:\n","            return self.__getitem__((idx + 1) % len(self))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        h, w, _ = image.shape\n","\n","        if not self.is_test:\n","            mask = np.zeros((h, w), dtype=np.float32)\n","            words = self.data[img_name].get('words', {})\n","            for word_id in words:\n","                points = np.array(words[word_id]['points'], dtype=np.int32)\n","                cv2.fillPoly(mask, [points], 1)\n","            if self.transform:\n","                augmented = self.transform(image=image, mask=mask)\n","                return augmented['image'], augmented['mask']\n","        else:\n","            if self.transform:\n","                augmented = self.transform(image=image)\n","                return augmented['image'], img_name, (h, w)\n","        return image\n","\n","# --- 4. ì „ì²˜ë¦¬ ---\n","train_transform = A.Compose([\n","    A.Resize(RESIZE_TARGET, RESIZE_TARGET),\n","    A.HorizontalFlip(p=0.5),\n","    A.RandomBrightnessContrast(p=0.2),\n","    A.Normalize(),\n","    ToTensorV2()\n","])\n","\n","val_transform = A.Compose([\n","    A.Resize(RESIZE_TARGET, RESIZE_TARGET),\n","    A.Normalize(),\n","    ToTensorV2()\n","])\n","\n","# --- 5. ë©”ì¸ í•¨ìˆ˜ ---\n","def main():\n","    # GPU ì •ë³´ ì¶œë ¥\n","    print(f\"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","    train_ds = ReceiptDataset(TRAIN_IMG_DIR, TRAIN_JSON, transform=train_transform)\n","    val_ds = ReceiptDataset(VAL_IMG_DIR, VAL_JSON, transform=val_transform)\n","\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=4,\n","        pin_memory=True,\n","        drop_last=True,\n","        persistent_workers=True  # A100ì—ì„œ íš¨ê³¼ì \n","    )\n","    val_loader = DataLoader(\n","        val_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=4,\n","        pin_memory=True,\n","        persistent_workers=True\n","    )\n","\n","    model = smp.UnetPlusPlus(\n","        encoder_name=\"resnet34\",\n","        encoder_weights=\"imagenet\",\n","        in_channels=3,\n","        classes=1\n","    ).to(DEVICE)\n","\n","    # ëª¨ë¸ì„ channels_last í¬ë§·ìœ¼ë¡œ (A100 í…ì„œì½”ì–´ ìµœì í™”)\n","    model = model.to(memory_format=torch.channels_last)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","    criterion = smp.losses.DiceLoss(mode='binary')\n","    scaler = GradScaler('cuda')\n","\n","    best_val_loss = float('inf')\n","\n","    print(f\"ğŸš€ A100 í•™ìŠµ ì‹œì‘\")\n","    print(f\"   Batch: {BATCH_SIZE} Ã— Accumulation: {ACCUMULATION_STEPS} = ì‹¤íš¨ {BATCH_SIZE * ACCUMULATION_STEPS}\")\n","    print(f\"   Image Size: {RESIZE_TARGET}Ã—{RESIZE_TARGET}\")\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        model.train()\n","        train_loss = 0\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\")\n","        for step, (images, masks) in pbar:\n","            # channels_last í¬ë§· ì ìš©\n","            images = images.to(DEVICE, non_blocking=True, memory_format=torch.channels_last)\n","            masks = masks.to(DEVICE, non_blocking=True).unsqueeze(1)\n","\n","            with autocast('cuda', dtype=torch.bfloat16):  # A100ì€ bfloat16ì´ ë” ì•ˆì •ì \n","                outputs = model(images)\n","                loss = criterion(outputs, masks) / ACCUMULATION_STEPS\n","\n","            scaler.scale(loss).backward()\n","\n","            if (step + 1) % ACCUMULATION_STEPS == 0:\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad(set_to_none=True)\n","\n","            train_loss += loss.item() * ACCUMULATION_STEPS\n","            pbar.set_postfix({'loss': f'{loss.item() * ACCUMULATION_STEPS:.4f}'})\n","\n","        scheduler.step()\n","        torch.cuda.empty_cache()\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for images, masks in tqdm(val_loader, desc=\"Val\", leave=False):\n","                images = images.to(DEVICE, non_blocking=True, memory_format=torch.channels_last)\n","                masks = masks.to(DEVICE, non_blocking=True).unsqueeze(1)\n","                with autocast('cuda', dtype=torch.bfloat16):\n","                    outputs = model(images)\n","                    loss = criterion(outputs, masks)\n","                val_loss += loss.item()\n","\n","        avg_val = val_loss / len(val_loader)\n","        avg_train = train_loss / len(train_loader)\n","        lr = scheduler.get_last_lr()[0]\n","        print(f\"ğŸ“Š Epoch {epoch}: Train {avg_train:.4f}, Val {avg_val:.4f}, LR {lr:.6f}\")\n","\n","        if avg_val < best_val_loss:\n","            best_val_loss = avg_val\n","            torch.save(model.state_dict(), 'best_model.pth')\n","            print(f\"   âœ… Best model saved!\")\n","\n","        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\n","        mem_used = torch.cuda.max_memory_allocated() / 1e9\n","        print(f\"   ğŸ’¾ Peak Memory: {mem_used:.1f} GB\")\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f2b3c301d7dd4eecad149bf050d38525","26b31214c0df448cabf47f6e2f4df190","ed518be5c0cd4c3e9b7de5dff825a411","b772f5f18ea24f098a1e72d63f6f53a6","22a27694a7bd4f22bb87ad8aab988d55","73a9d6e1e1564f0a98b1396003781550","7924213a774a4f7ca093ea3db6281a65","2b3268cb60d04b46a385368dad24a6ca","2c896f1ab4f24c7b9892551a22e6c260","521ddf52a391496e8d9d7d36f9ffc1b1","db6c494189274e508bb6bdb7cdffe622","c935120d07bf4223b255001e0faae625","6756c59710704765ba7dd338a9d80352","afa7eb5e4ed049609dad51bb3fc24999","21c493a0cf094b14a14337b197b1557b","8fa10f68a72f44958e6a63c12d4863aa","1678f2a396464df59ae4b9863ab84361","33c6305305b14661ad5dfd29e1a56617","72c781bc1b7a4f50b5da3a7f4262ff06","f6f12444c4084d0092e0b34861290cc9","2d8b86ac5f1d449a8e2f17ef08295f4b","ba6654d5f475420d83b80a6a9b5b9fbf"]},"id":"gnF1gApXggLA","executionInfo":{"status":"ok","timestamp":1769588178681,"user_tz":-540,"elapsed":1441866,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"2f984b86-bcb8-4944-a181-a2fe2b2e0e29"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ–¥ï¸ GPU: NVIDIA A100-SXM4-40GB\n","ğŸ’¾ GPU Memory: 42.5 GB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b3c301d7dd4eecad149bf050d38525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c935120d07bf4223b255001e0faae625"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ğŸš€ A100 í•™ìŠµ ì‹œì‘\n","   Batch: 16 Ã— Accumulation: 2 = ì‹¤íš¨ 32\n","   Image Size: 512Ã—512\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:55<00:00,  1.76it/s, loss=0.2736]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 1: Train 0.3972, Val 0.2928, LR 0.000099\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 11.1 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:06<00:00,  3.06it/s, loss=0.1941]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 2: Train 0.2489, Val 0.2124, LR 0.000098\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.10it/s, loss=0.1577]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 3: Train 0.1880, Val 0.1658, LR 0.000095\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.1434]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 4: Train 0.1529, Val 0.1405, LR 0.000090\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.1220]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 5: Train 0.1326, Val 0.1247, LR 0.000085\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.12it/s, loss=0.1067]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 6: Train 0.1189, Val 0.1141, LR 0.000079\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.1027]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 7: Train 0.1095, Val 0.1086, LR 0.000073\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.12it/s, loss=0.1023]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 8: Train 0.1040, Val 0.1037, LR 0.000065\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.1124]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 9: Train 0.0981, Val 0.0992, LR 0.000058\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.12it/s, loss=0.0935]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 10: Train 0.0934, Val 0.0979, LR 0.000050\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.0854]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 11: Train 0.0894, Val 0.0960, LR 0.000042\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.0848]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 12: Train 0.0864, Val 0.0950, LR 0.000035\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.0781]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 13: Train 0.0839, Val 0.0933, LR 0.000027\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.12it/s, loss=0.0764]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 14: Train 0.0818, Val 0.0924, LR 0.000021\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.12it/s, loss=0.0894]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 15: Train 0.0799, Val 0.0917, LR 0.000015\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.0743]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 16: Train 0.0786, Val 0.0918, LR 0.000010\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.12it/s, loss=0.0785]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 17: Train 0.0774, Val 0.0915, LR 0.000005\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.0694]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 18: Train 0.0768, Val 0.0911, LR 0.000002\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.12it/s, loss=0.0719]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 19: Train 0.0761, Val 0.0911, LR 0.000001\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204/204 [01:05<00:00,  3.11it/s, loss=0.0889]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“Š Epoch 20: Train 0.0760, Val 0.0911, LR 0.000000\n","   âœ… Best model saved!\n","   ğŸ’¾ Peak Memory: 8.6 GB\n","âœ… í•™ìŠµ ì™„ë£Œ!\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.amp import autocast\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import segmentation_models_pytorch as smp\n","from tqdm import tqdm\n","\n","# --- ì„¤ì • ---\n","BASE_PATH = './data/datasets'\n","TEST_IMG_DIR = os.path.join(BASE_PATH, 'images/test')\n","TEST_JSON = os.path.join(BASE_PATH, 'jsons/test.json')\n","SAMPLE_SUB = os.path.join(BASE_PATH, 'sample_submission.csv')\n","MODEL_PATH = 'best_model.pth'\n","OUTPUT_CSV = 'submission.csv'\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","RESIZE_TARGET = 512\n","BATCH_SIZE = 16\n","THRESHOLD = 0.5\n","\n","# --- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ---\n","class TestDataset(Dataset):\n","    def __init__(self, img_dir, json_path, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        with open(json_path, 'r', encoding='utf-8') as f:\n","            self.data = json.load(f)['images']\n","        self.image_names = list(self.data.keys())\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.img_dir, img_name)\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        h, w, _ = image.shape\n","\n","        if self.transform:\n","            augmented = self.transform(image=image)\n","            return augmented['image'], img_name, (h, w)\n","        return image, img_name, (h, w)\n","\n","# --- ë§ˆìŠ¤í¬ â†’ í´ë¦¬ê³¤ ë³€í™˜ ---\n","def mask_to_polygons(mask, min_area=100):\n","    polygons = []\n","    contours, _ = cv2.findContours(\n","        mask.astype(np.uint8),\n","        cv2.RETR_EXTERNAL,\n","        cv2.CHAIN_APPROX_SIMPLE\n","    )\n","\n","    for contour in contours:\n","        area = cv2.contourArea(contour)\n","        if area < min_area:\n","            continue\n","\n","        epsilon = 0.02 * cv2.arcLength(contour, True)\n","        approx = cv2.approxPolyDP(contour, epsilon, True)\n","\n","        if len(approx) >= 4:\n","            points = approx.reshape(-1, 2).tolist()\n","            polygons.append(points)\n","\n","    return polygons\n","\n","# --- í´ë¦¬ê³¤ â†’ ë¬¸ìì—´ ë³€í™˜ ---\n","def polygons_to_string(polygons):\n","    \"\"\"[[x1,y1],[x2,y2],...] í˜•íƒœë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\"\"\"\n","    if not polygons:\n","        return \"\"\n","\n","    result = []\n","    for poly in polygons:\n","        # [[x1,y1],[x2,y2],...] â†’ \"x1 y1 x2 y2 ...\"  (ê³µë°± êµ¬ë¶„!)\n","        coords = []\n","        for point in poly:\n","            coords.append(f\"{int(point[0])} {int(point[1])}\")\n","        result.append(\" \".join(coords))\n","\n","    # ì—¬ëŸ¬ í´ë¦¬ê³¤ì€ | ë¡œ êµ¬ë¶„\n","    return \"|\".join(result)\n","\n","# --- ì¶”ë¡  í•¨ìˆ˜ ---\n","def inference():\n","    print(f\"ğŸ” ì¶”ë¡  ì‹œì‘\")\n","\n","    # sample_submission ì½ê¸°\n","    sample_df = pd.read_csv(SAMPLE_SUB)\n","    print(f\"ğŸ“„ Sample submission í˜•ì‹:\")\n","    print(f\"   Columns: {list(sample_df.columns)}\")\n","    print(f\"   Shape: {sample_df.shape}\")\n","    print(f\"   Sample:\\n{sample_df.head(3)}\")\n","\n","    # ì „ì²˜ë¦¬\n","    test_transform = A.Compose([\n","        A.Resize(RESIZE_TARGET, RESIZE_TARGET),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ])\n","\n","    # ë°ì´í„° ë¡œë”\n","    test_ds = TestDataset(TEST_IMG_DIR, TEST_JSON, transform=test_transform)\n","    test_loader = DataLoader(\n","        test_ds,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=2,\n","        pin_memory=True\n","    )\n","\n","    # ëª¨ë¸ ë¡œë“œ\n","    model = smp.UnetPlusPlus(\n","        encoder_name=\"resnet34\",\n","        encoder_weights=None,\n","        in_channels=3,\n","        classes=1\n","    ).to(DEVICE)\n","    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","    model = model.to(memory_format=torch.channels_last)\n","    model.eval()\n","\n","    # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬\n","    predictions = {}\n","\n","    with torch.no_grad():\n","        for images, img_names, orig_sizes in tqdm(test_loader, desc=\"Inference\"):\n","            images = images.to(DEVICE, memory_format=torch.channels_last)\n","\n","            with autocast('cuda', dtype=torch.bfloat16):\n","                outputs = model(images)\n","\n","            preds = torch.sigmoid(outputs).float().cpu().numpy()\n","\n","            for i, img_name in enumerate(img_names):\n","                orig_h, orig_w = orig_sizes[0][i].item(), orig_sizes[1][i].item()\n","\n","                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n","                pred_mask = preds[i, 0]\n","                pred_mask = cv2.resize(pred_mask, (orig_w, orig_h))\n","\n","                # ì´ì§„í™”\n","                binary_mask = (pred_mask > THRESHOLD).astype(np.uint8)\n","\n","                # í´ë¦¬ê³¤ ì¶”ì¶œ\n","                polygons = mask_to_polygons(binary_mask)\n","\n","                # ì´ë¯¸ì§€ëª…ì„ í‚¤ë¡œ ì €ì¥\n","                predictions[img_name] = polygons\n","\n","    # --- sample_submission í˜•ì‹ì— ë§ì¶° ì €ì¥ ---\n","    result_df = sample_df.copy()\n","\n","    # ì»¬ëŸ¼ëª… í™•ì¸ í›„ ìë™ ë§¤í•‘\n","    id_col = sample_df.columns[0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ = ID/ì´ë¯¸ì§€ëª…\n","    pred_col = sample_df.columns[1] if len(sample_df.columns) > 1 else 'prediction'  # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ = ì˜ˆì¸¡ê°’\n","\n","    print(f\"\\nğŸ“ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n","    print(f\"   ID ì»¬ëŸ¼: {id_col}\")\n","    print(f\"   ì˜ˆì¸¡ ì»¬ëŸ¼: {pred_col}\")\n","\n","    for idx, row in result_df.iterrows():\n","        img_id = row[id_col]\n","\n","        # ì´ë¯¸ì§€ëª… ë§¤ì¹­ (í™•ì¥ì ìœ ë¬´ì— ë”°ë¼ ì²˜ë¦¬)\n","        if img_id in predictions:\n","            polygons = predictions[img_id]\n","        elif f\"{img_id}.jpg\" in predictions:\n","            polygons = predictions[f\"{img_id}.jpg\"]\n","        elif f\"{img_id}.png\" in predictions:\n","            polygons = predictions[f\"{img_id}.png\"]\n","        else:\n","            polygons = []\n","\n","        # í´ë¦¬ê³¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥\n","        result_df.at[idx, pred_col] = polygons_to_string(polygons)\n","\n","    # CSV ì €ì¥\n","    result_df.to_csv(OUTPUT_CSV, index=False)\n","\n","    print(f\"\\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_CSV}\")\n","    print(f\"   ì´ ì´ë¯¸ì§€: {len(result_df)}ê°œ\")\n","    print(f\"\\nğŸ“Š ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:\")\n","    print(result_df.head(5))\n","\n","if __name__ == \"__main__\":\n","    inference()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1uxJ13T0hseS","executionInfo":{"status":"ok","timestamp":1769590075616,"user_tz":-540,"elapsed":3859,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"69a3d3e7-21c0-4533-f799-a8f46a45d09b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” ì¶”ë¡  ì‹œì‘\n","ğŸ“„ Sample submission í˜•ì‹:\n","   Columns: ['filename', 'polygons']\n","   Shape: (413, 2)\n","   Sample:\n","                                   filename  \\\n","0  drp.en_ko.in_house.selectstar_003883.jpg   \n","1  drp.en_ko.in_house.selectstar_000132.jpg   \n","2  drp.en_ko.in_house.selectstar_001796.jpg   \n","\n","                                            polygons  \n","0  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n","1  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n","2  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n"]},{"output_type":"stream","name":"stderr","text":["Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:03<00:00,  8.08it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸ“ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n","   ID ì»¬ëŸ¼: filename\n","   ì˜ˆì¸¡ ì»¬ëŸ¼: polygons\n","\n","âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: submission.csv\n","   ì´ ì´ë¯¸ì§€: 413ê°œ\n","\n","ğŸ“Š ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:\n","                                   filename  \\\n","0  drp.en_ko.in_house.selectstar_003883.jpg   \n","1  drp.en_ko.in_house.selectstar_000132.jpg   \n","2  drp.en_ko.in_house.selectstar_001796.jpg   \n","3  drp.en_ko.in_house.selectstar_001202.jpg   \n","4  drp.en_ko.in_house.selectstar_002858.jpg   \n","\n","                                            polygons  \n","0  392 1145 474 1149 493 1140 439 1135 398 1139|4...  \n","1  648 1131 648 1176 730 1178 730 1137|334 1023 3...  \n","2  236 1052 248 1174 469 1181 508 1131 633 1184 5...  \n","3  513 1223 529 1226 538 1212 657 1210 662 1202 5...  \n","4  477 1216 476 1228 555 1233 559 1226 556 1212|3...  \n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('./data/datasets/sample_submission.csv')\n","print(df.head())\n","print(df.columns.tolist())\n","print(df.dtypes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVGNO0HVsA23","executionInfo":{"status":"ok","timestamp":1769590023519,"user_tz":-540,"elapsed":7,"user":{"displayName":"Seo Yeon Moon","userId":"13518922635864643615"}},"outputId":"bf208f04-aab9-44a7-fbb1-e238e3f2b940"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["                                   filename  \\\n","0  drp.en_ko.in_house.selectstar_003883.jpg   \n","1  drp.en_ko.in_house.selectstar_000132.jpg   \n","2  drp.en_ko.in_house.selectstar_001796.jpg   \n","3  drp.en_ko.in_house.selectstar_001202.jpg   \n","4  drp.en_ko.in_house.selectstar_002858.jpg   \n","\n","                                            polygons  \n","0  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n","1  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n","2  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n","3  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n","4  10 50 100 50 100 150 10 150|110 150 200 150 20...  \n","['filename', 'polygons']\n","filename    object\n","polygons    object\n","dtype: object\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Zng5TEJ3yOTJ"},"execution_count":null,"outputs":[]}]}