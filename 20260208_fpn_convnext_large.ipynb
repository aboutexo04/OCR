{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "!wget -O data.tar.gz \"https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000377/data/data.tar.gz\"\n",
    "!tar -xzf data.tar.gz\n",
    "!pip install -q segmentation-models-pytorch albumentations opencv-python-headless pyclipper shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Train (FPN + ConvNeXt-Large, EMA, Val monitoring)\n",
    "import os, json, cv2, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import pyclipper\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "import ssl, warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda')\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}, VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n",
    "\n",
    "# === Config ===\n",
    "ENCODER = 'tu-convnext_large'  # Base -> Large (3x params)\n",
    "SZ = 1024\n",
    "BS = 4\n",
    "ACCUM = 2       # effective batch = 8\n",
    "EPOCHS = 15     # Large model converges faster\n",
    "LR = 5e-4\n",
    "SHRINK_RATIO = 0.4\n",
    "EMA_DECAY = 0.999\n",
    "UNCLIP_RATIO = 3.0\n",
    "BOX_THRESH = 0.3\n",
    "\n",
    "BASE = './data/datasets'\n",
    "PSEUDO = './data/pseudo_label'\n",
    "MODEL_PATH = 'fpn_convnext_large_best.pth'\n",
    "\n",
    "# === Dataset ===\n",
    "class DBDataset(Dataset):\n",
    "    def __init__(self, img_dir, json_path, transform=None, mode='train'):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.data = {}\n",
    "        self.image_names = []\n",
    "        if json_path and os.path.exists(json_path):\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                self.data = json.load(f)['images']\n",
    "            self.image_names = list(self.data.keys())\n",
    "        elif json_path:\n",
    "            alt = json_path.replace('train.json', 'val.json')\n",
    "            if os.path.exists(alt):\n",
    "                with open(alt, 'r', encoding='utf-8') as f:\n",
    "                    self.data = json.load(f)['images']\n",
    "                self.image_names = list(self.data.keys())\n",
    "\n",
    "    def __len__(self): return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.image_names[idx]\n",
    "        image = cv2.imread(os.path.join(self.img_dir, name))\n",
    "        if image is None: return self.__getitem__((idx + 1) % len(self))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w = image.shape[:2]\n",
    "        mask = np.zeros((h, w), dtype=np.float32)\n",
    "\n",
    "        if self.mode != 'test' and 'words' in self.data[name]:\n",
    "            for w_info in self.data[name]['words'].values():\n",
    "                pts = np.array(w_info['points'], dtype=np.int32)\n",
    "                try:\n",
    "                    poly = Polygon(pts)\n",
    "                    if poly.area > 0 and poly.length > 0:\n",
    "                        d = poly.area * (1 - SHRINK_RATIO**2) / poly.length\n",
    "                        pco = pyclipper.PyclipperOffset()\n",
    "                        pco.AddPath(pts, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "                        shrunk = pco.Execute(-d)\n",
    "                        if shrunk:\n",
    "                            cv2.fillPoly(mask, [np.array(shrunk[0], dtype=np.int32)], 1)\n",
    "                        else:\n",
    "                            cv2.fillPoly(mask, [pts], 1)\n",
    "                except:\n",
    "                    cv2.fillPoly(mask, [pts], 1)\n",
    "\n",
    "        if self.transform:\n",
    "            aug = self.transform(image=image, mask=mask)\n",
    "            image, mask = aug['image'], aug['mask']\n",
    "\n",
    "        if self.mode == 'test': return image, name, (h, w)\n",
    "        return image, mask\n",
    "\n",
    "# === EMA ===\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad: self.shadow[n] = p.data.clone()\n",
    "    def update(self, model):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[n].mul_(self.decay).add_(p.data, alpha=1-self.decay)\n",
    "    def apply(self, model):\n",
    "        self.backup = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.backup[n] = p.data.clone()\n",
    "                p.data.copy_(self.shadow[n])\n",
    "    def restore(self, model):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad: p.data.copy_(self.backup[n])\n",
    "        self.backup = {}\n",
    "\n",
    "# === Augmentation ===\n",
    "train_tf = A.Compose([\n",
    "    A.Resize(SZ, SZ),\n",
    "    A.Perspective(scale=(0.05, 0.1), p=0.5),\n",
    "    A.Rotate(limit=20, p=0.5),\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "    A.GaussNoise(p=0.15),\n",
    "    A.Normalize(), ToTensorV2()\n",
    "])\n",
    "val_tf = A.Compose([A.Resize(SZ, SZ), A.Normalize(), ToTensorV2()])\n",
    "\n",
    "# === Data ===\n",
    "datasets = []\n",
    "ds_train = DBDataset(os.path.join(BASE, 'images/train'),\n",
    "                     os.path.join(BASE, 'jsons/train.json'), train_tf)\n",
    "if len(ds_train) > 0: datasets.append(ds_train)\n",
    "\n",
    "ds_val = DBDataset(os.path.join(BASE, 'images/val'),\n",
    "                   os.path.join(BASE, 'jsons/val.json'), train_tf)\n",
    "if len(ds_val) > 0: datasets.append(ds_val)\n",
    "\n",
    "for folder in ['sroie', 'cord-v2', 'wildreceipt']:\n",
    "    p_img = os.path.join(PSEUDO, folder, 'images')\n",
    "    p_json = os.path.join(PSEUDO, folder, 'train.json')\n",
    "    if os.path.exists(p_json) and os.path.exists(p_img):\n",
    "        ds = DBDataset(p_img, p_json, train_tf)\n",
    "        if len(ds) > 0: datasets.append(ds)\n",
    "\n",
    "full_ds = ConcatDataset(datasets)\n",
    "train_dl = DataLoader(full_ds, BS, shuffle=True, num_workers=4,\n",
    "                      pin_memory=True, drop_last=True, persistent_workers=True)\n",
    "\n",
    "val_ds = DBDataset(os.path.join(BASE, 'images/val'),\n",
    "                   os.path.join(BASE, 'jsons/val.json'), val_tf)\n",
    "val_dl = DataLoader(val_ds, BS, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(full_ds)}, Val monitor: {len(val_ds)}\")\n",
    "\n",
    "# === Model ===\n",
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights='imagenet',\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    ").to(DEVICE)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"FPN + {ENCODER}: {n_params:.1f}M params\")\n",
    "\n",
    "# === Training ===\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LR, epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_dl)//ACCUM, pct_start=0.1)\n",
    "\n",
    "dice_fn = smp.losses.DiceLoss(mode='binary')\n",
    "bce_fn = smp.losses.SoftBCEWithLogitsLoss()\n",
    "scaler = GradScaler('cuda')\n",
    "ema = EMA(model, EMA_DECAY)\n",
    "\n",
    "best_val = float('inf')\n",
    "patience = 0\n",
    "\n",
    "print(f\"=== Training: {SZ}x{SZ}, BS={BS}x{ACCUM}={BS*ACCUM}, Epochs={EPOCHS} ===\")\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    t_loss = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for i, (imgs, msks) in enumerate(tqdm(train_dl, desc=f\"E{ep}/{EPOCHS}\")):\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        msks = msks.to(DEVICE, non_blocking=True).unsqueeze(1)\n",
    "\n",
    "        with autocast('cuda', dtype=torch.bfloat16):\n",
    "            preds = model(imgs)\n",
    "            loss = (dice_fn(preds, msks) + bce_fn(preds, msks)) / ACCUM\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i+1) % ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            ema.update(model)\n",
    "\n",
    "        t_loss += loss.item() * ACCUM\n",
    "\n",
    "    # Val (EMA)\n",
    "    ema.apply(model)\n",
    "    model.eval()\n",
    "    v_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, msks in val_dl:\n",
    "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "            msks = msks.to(DEVICE, non_blocking=True).unsqueeze(1)\n",
    "            with autocast('cuda', dtype=torch.bfloat16):\n",
    "                preds = model(imgs)\n",
    "                v_loss += (dice_fn(preds, msks) + bce_fn(preds, msks)).item()\n",
    "\n",
    "    avg_t = t_loss / len(train_dl)\n",
    "    avg_v = v_loss / len(val_dl)\n",
    "    print(f\"E{ep}: train={avg_t:.4f} val={avg_v:.4f} lr={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if avg_v < best_val:\n",
    "        best_val = avg_v\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(f\"  -> Best! (val={avg_v:.4f})\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= 8:\n",
    "            print(\"  -> Early stopping\")\n",
    "            ema.restore(model)\n",
    "            break\n",
    "\n",
    "    ema.restore(model)\n",
    "    mem = torch.cuda.max_memory_allocated()/1e9\n",
    "    print(f\"  -> VRAM: {mem:.1f}GB\")\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Done. Best val={best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Inference (0.9553 settings: thresh=0.3, unclip=3.0)\n",
    "def unclip(box, ratio):\n",
    "    poly = Polygon(box)\n",
    "    if poly.area <= 0 or poly.length <= 0:\n",
    "        return [box.tolist() if hasattr(box, 'tolist') else box]\n",
    "    d = poly.area * ratio / poly.length\n",
    "    pco = pyclipper.PyclipperOffset()\n",
    "    pco.AddPath([(int(p[0]), int(p[1])) for p in box],\n",
    "                pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "    expanded = pco.Execute(d)\n",
    "    return expanded if expanded else [box.tolist() if hasattr(box, 'tolist') else box]\n",
    "\n",
    "def run_inference(box_thresh, unclip_ratio, output_csv):\n",
    "    print(f\"=== Inference (thresh={box_thresh}, unclip={unclip_ratio}) ===\")\n",
    "\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    test_ds = DBDataset(os.path.join(BASE, 'images/test'),\n",
    "                        os.path.join(BASE, 'jsons/test.json'), val_tf, mode='test')\n",
    "    test_dl = DataLoader(test_ds, BS, shuffle=False, num_workers=4)\n",
    "\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for imgs, names, (ohs, ows) in tqdm(test_dl, desc=\"Inference\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            with autocast('cuda', dtype=torch.bfloat16):\n",
    "                p1 = torch.sigmoid(model(imgs))\n",
    "                p2 = torch.sigmoid(model(torch.flip(imgs, [3])))\n",
    "                preds = (p1 + torch.flip(p2, [3])) / 2\n",
    "            preds = preds.float().cpu().numpy()\n",
    "\n",
    "            for i, name in enumerate(names):\n",
    "                oh, ow = ohs[i].item(), ows[i].item()\n",
    "                mask = cv2.resize(preds[i][0], (ow, oh))\n",
    "                binary = (mask > box_thresh).astype(np.uint8)\n",
    "\n",
    "                contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL,\n",
    "                                               cv2.CHAIN_APPROX_SIMPLE)\n",
    "                polys = []\n",
    "                for cnt in contours:\n",
    "                    if cv2.contourArea(cnt) < 30: continue\n",
    "                    eps = 0.003 * cv2.arcLength(cnt, True)\n",
    "                    approx = cv2.approxPolyDP(cnt, eps, True)\n",
    "                    if len(approx) < 3: continue\n",
    "                    pts = approx.reshape(-1, 2)\n",
    "                    try:\n",
    "                        expanded = unclip(pts, unclip_ratio)\n",
    "                        final = np.array(expanded[0])\n",
    "                        polys.append(final.reshape(-1).tolist())\n",
    "                    except:\n",
    "                        polys.append(pts.reshape(-1).tolist())\n",
    "\n",
    "                results[name] = \"|\" .join(\" \".join(map(str, p)) for p in polys)\n",
    "\n",
    "    df = pd.read_csv(os.path.join(BASE, 'sample_submission.csv'))\n",
    "    df['polygons'] = df['filename'].map(results).fillna(\"\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    counts = df['polygons'].apply(lambda x: len(x.split('|')) if x else 0)\n",
    "    print(f\"Saved: {output_csv}\")\n",
    "    print(f\"Avg poly/img: {counts.mean():.1f}, Min: {counts.min()}, Max: {counts.max()}\")\n",
    "\n",
    "# 기본 추론 (0.9553 달성 설정)\n",
    "run_inference(box_thresh=0.3, unclip_ratio=3.0, output_csv='submission_fpn_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Re-inference v2 (thresh=0.25)\n",
    "run_inference(box_thresh=0.25, unclip_ratio=3.0, output_csv='submission_fpn_large_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Re-inference v3 (thresh=0.2, unclip=3.5)\n",
    "run_inference(box_thresh=0.2, unclip_ratio=3.5, output_csv='submission_fpn_large_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp fpn_convnext_large_best.pth /content/drive/MyDrive/fpn_convnext_large_best.pth\n",
    "print(\"Saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Ensemble (Base + Large) - 두 모델 예측 평균\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_MODEL_PATH = '/content/drive/MyDrive/OCR_Best_Model/DBNet_ConvNeXt_FPN_final.pth'\n",
    "LARGE_MODEL_PATH = 'fpn_convnext_large_best.pth'\n",
    "\n",
    "# Base 모델 로드\n",
    "model_base = smp.FPN(encoder_name='tu-convnext_base', in_channels=3, classes=1).to(DEVICE)\n",
    "model_base.load_state_dict(torch.load(BASE_MODEL_PATH, map_location=DEVICE))\n",
    "model_base.eval()\n",
    "\n",
    "# Large 모델 로드\n",
    "model_large = smp.FPN(encoder_name='tu-convnext_large', in_channels=3, classes=1).to(DEVICE)\n",
    "model_large.load_state_dict(torch.load(LARGE_MODEL_PATH, map_location=DEVICE))\n",
    "model_large.eval()\n",
    "\n",
    "print(\"Both models loaded!\")\n",
    "\n",
    "def ensemble_inference(box_thresh, unclip_ratio, output_csv):\n",
    "    print(f\"=== Ensemble Inference (thresh={box_thresh}, unclip={unclip_ratio}) ===\")\n",
    "\n",
    "    test_ds = DBDataset(os.path.join(BASE, 'images/test'),\n",
    "                        os.path.join(BASE, 'jsons/test.json'), val_tf, mode='test')\n",
    "    test_dl = DataLoader(test_ds, BS, shuffle=False, num_workers=4)\n",
    "\n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for imgs, names, (ohs, ows) in tqdm(test_dl, desc=\"Ensemble\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            with autocast('cuda', dtype=torch.bfloat16):\n",
    "                # Base model (+ TTA)\n",
    "                b1 = torch.sigmoid(model_base(imgs))\n",
    "                b2 = torch.sigmoid(model_base(torch.flip(imgs, [3])))\n",
    "                pred_base = (b1 + torch.flip(b2, [3])) / 2\n",
    "\n",
    "                # Large model (+ TTA)\n",
    "                l1 = torch.sigmoid(model_large(imgs))\n",
    "                l2 = torch.sigmoid(model_large(torch.flip(imgs, [3])))\n",
    "                pred_large = (l1 + torch.flip(l2, [3])) / 2\n",
    "\n",
    "                # Ensemble: average\n",
    "                preds = (pred_base + pred_large) / 2\n",
    "\n",
    "            preds = preds.float().cpu().numpy()\n",
    "\n",
    "            for i, name in enumerate(names):\n",
    "                oh, ow = ohs[i].item(), ows[i].item()\n",
    "                mask = cv2.resize(preds[i][0], (ow, oh))\n",
    "                binary = (mask > box_thresh).astype(np.uint8)\n",
    "\n",
    "                contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL,\n",
    "                                               cv2.CHAIN_APPROX_SIMPLE)\n",
    "                polys = []\n",
    "                for cnt in contours:\n",
    "                    if cv2.contourArea(cnt) < 30: continue\n",
    "                    eps = 0.003 * cv2.arcLength(cnt, True)\n",
    "                    approx = cv2.approxPolyDP(cnt, eps, True)\n",
    "                    if len(approx) < 3: continue\n",
    "                    pts = approx.reshape(-1, 2)\n",
    "                    try:\n",
    "                        expanded = unclip(pts, unclip_ratio)\n",
    "                        final = np.array(expanded[0])\n",
    "                        polys.append(final.reshape(-1).tolist())\n",
    "                    except:\n",
    "                        polys.append(pts.reshape(-1).tolist())\n",
    "\n",
    "                results[name] = \"|\".join(\" \".join(map(str, p)) for p in polys)\n",
    "\n",
    "    df = pd.read_csv(os.path.join(BASE, 'sample_submission.csv'))\n",
    "    df['polygons'] = df['filename'].map(results).fillna(\"\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    counts = df['polygons'].apply(lambda x: len(x.split('|')) if x else 0)\n",
    "    print(f\"Saved: {output_csv}\")\n",
    "    print(f\"Avg poly/img: {counts.mean():.1f}, Min: {counts.min()}, Max: {counts.max()}\")\n",
    "\n",
    "# 0.9553 달성 설정으로 앙상블\n",
    "ensemble_inference(box_thresh=0.3, unclip_ratio=3.0, output_csv='submission_ensemble.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
